{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "In this notebook we perform binary logistic regression on the diabetes dataset. Our analysis involves the following steps:\n",
    "- Initial Exploration\n",
    "- Exploring Resampling Methods\n",
    "- Hyperparamter Tuning and Cross Validation\n",
    "- Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))\n",
    "from data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (177576, 21)\n",
      "y_train shape: (177576,)\n",
      "X_val shape: (25875, 21)\n",
      "y_val shape: (25875,)\n",
      "X_test shape: (50229, 21)\n",
      "y_test shape: (50229,)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_loader = DataLoader()\n",
    "X_train, y_train  = data_loader.training_data\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8646    0.9711    0.9148     21797\n",
      "         1.0     0.5477    0.1871    0.2789      4078\n",
      "\n",
      "    accuracy                         0.8475     25875\n",
      "   macro avg     0.7062    0.5791    0.5968     25875\n",
      "weighted avg     0.8147    0.8475    0.8145     25875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train first logistic regression model\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling Methods\n",
    "We applied resampling methods to address the issue of class imbalance in our dataset because imbalanced data can lead to biased machine learning models that struggle to predict the minority class effectively. By balancing the class distribution, we aim to improve model performance and ensure fair representation. Specifically, we used the following methods:\n",
    "\n",
    "- **Random Undersampling**: This method involves randomly removing samples from the majority class to balance the class distribution.\n",
    "\n",
    "- **Random Oversampling**: This method involves randomly duplicating samples from the minority class to balance the class distribution.\n",
    "\n",
    "- **SMOTE Oversampling**: Synthetic Minority Over-sampling Technique (SMOTE) generates synthetic samples for the minority class by interpolating between existing minority class samples.\n",
    "\n",
    "- **SMOTE Tomek**: This method combines SMOTE oversampling with Tomek links, which are pairs of samples from different classes that are close to each other. By removing Tomek links after applying SMOTE, this method helps in cleaning the boundary between classes, leading to a more balanced and cleaner dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_undersampling_random shape: (55968, 21)\n",
      "y_train_undersampling_random shape: (55968,)\n",
      "X_val shape: (25875, 21)\n",
      "y_val shape: (25875,)\n",
      "X_test shape: (50229, 21)\n",
      "y_test shape: (50229,)\n"
     ]
    }
   ],
   "source": [
    "# test random undersampling\n",
    "X_train_undersampling_random, y_train_undersampling_random = data_loader.training_data_undersampling_random\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data\n",
    "\n",
    "print(f\"X_train_undersampling_random shape: {X_train_undersampling_random.shape}\")\n",
    "print(f\"y_train_undersampling_random shape: {y_train_undersampling_random.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9437    0.7179    0.8155     21797\n",
      "         1.0     0.3384    0.7710    0.4703      4078\n",
      "\n",
      "    accuracy                         0.7263     25875\n",
      "   macro avg     0.6410    0.7445    0.6429     25875\n",
      "weighted avg     0.8483    0.7263    0.7611     25875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train_undersampling_random, y_train_undersampling_random)\n",
    "\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_oversampling_random shape: (299184, 21)\n",
      "y_train_oversampling_random shape: (299184,)\n",
      "X_val shape: (25875, 21)\n",
      "y_val shape: (25875,)\n",
      "X_test shape: (50229, 21)\n",
      "y_test shape: (50229,)\n"
     ]
    }
   ],
   "source": [
    "# test random oversampling\n",
    "X_train_oversampling_random, y_train_oversampling_random = data_loader.training_data_oversampling_random\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data\n",
    "\n",
    "print(f\"X_train_oversampling_random shape: {X_train_oversampling_random.shape}\")\n",
    "print(f\"y_train_oversampling_random shape: {y_train_oversampling_random.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9430    0.7199    0.8165     21797\n",
      "         1.0     0.3389    0.7675    0.4702      4078\n",
      "\n",
      "    accuracy                         0.7274     25875\n",
      "   macro avg     0.6410    0.7437    0.6433     25875\n",
      "weighted avg     0.8478    0.7274    0.7619     25875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train_oversampling_random, y_train_oversampling_random)\n",
    "\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_oversampling_smote shape: (299184, 21)\n",
      "y_train_oversampling_smote shape: (299184,)\n",
      "X_val shape: (25875, 21)\n",
      "y_val shape: (25875,)\n",
      "X_test shape: (50229, 21)\n",
      "y_test shape: (50229,)\n"
     ]
    }
   ],
   "source": [
    "# test smote oversampling\n",
    "X_train_oversampling_smote, y_train_oversampling_smote = data_loader.training_data_oversampling_smote\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data\n",
    "\n",
    "print(f\"X_train_oversampling_smote shape: {X_train_oversampling_smote.shape}\")\n",
    "print(f\"y_train_oversampling_smote shape: {y_train_oversampling_smote.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9399    0.7229    0.8172     21797\n",
      "         1.0     0.3370    0.7528    0.4655      4078\n",
      "\n",
      "    accuracy                         0.7276     25875\n",
      "   macro avg     0.6384    0.7378    0.6414     25875\n",
      "weighted avg     0.8448    0.7276    0.7618     25875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train_oversampling_smote, y_train_oversampling_smote)\n",
    "\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE Tomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_oversampling_smote shape: (298548, 21)\n",
      "y_train_oversampling_smote shape: (298548,)\n",
      "X_val shape: (25875, 21)\n",
      "y_val shape: (25875,)\n",
      "X_test shape: (50229, 21)\n",
      "y_test shape: (50229,)\n"
     ]
    }
   ],
   "source": [
    "X_train_oversampling_smote_tomek, y_train_oversampling_smote_tomek = data_loader.training_data_resampling_smote_tomek\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data\n",
    "\n",
    "print(f\"X_train_oversampling_smote shape: {X_train_oversampling_smote_tomek.shape}\")\n",
    "print(f\"y_train_oversampling_smote shape: {y_train_oversampling_smote_tomek.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9400    0.7226    0.8171     21797\n",
      "         1.0     0.3369    0.7533    0.4656      4078\n",
      "\n",
      "    accuracy                         0.7275     25875\n",
      "   macro avg     0.6384    0.7380    0.6413     25875\n",
      "weighted avg     0.8449    0.7275    0.7617     25875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train_oversampling_smote_tomek, y_train_oversampling_smote_tomek)\n",
    "\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion on Resampling Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a short conclusion that resampling methods can be used and why we should focus on recall. \n",
    "# also say that it makes sense to perform this in combination with cross validation and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning and Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "Best Parameters: {'C': 10, 'penalty': 'l2', 'solver': 'saga', 'tol': 0.1}\n",
      "Best Cross-Validation Recall: 0.22430469554876162\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning with Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'C': [0.1, 1, 10], 'penalty': ['l1'], 'solver': ['liblinear', 'saga'], 'tol': [1e-3, 1e-2, 1e-1]},\n",
    "    {'C': [0.1, 1, 10], 'penalty': ['l2'], 'solver': ['lbfgs', 'liblinear', 'newton-cg', 'saga'], 'tol': [1e-3, 1e-2, 1e-1]},\n",
    "    {'C': [0.1, 1, 10], 'penalty': ['elasticnet'], 'solver': ['saga'], 'tol': [1e-3, 1e-2, 1e-1], 'l1_ratio': [0.1, 0.5, 0.9]},\n",
    "    {'penalty': [None], 'solver': ['lbfgs', 'newton-cg', 'saga'], 'tol': [1e-3, 1e-2, 1e-1]}\n",
    "]\n",
    "\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='recall',  # scoring metric\n",
    "    n_jobs=-1,  # Use all available processors\n",
    "    verbose=1  # To see the progress\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Recall:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV tests every combination of the parameter grid, which can be very time-consuming, especially with a large number of hyperparameters. This exhaustive search limits the number of hyperparameters that can be tested within a suitable time frame. To address this, we will also apply RandomizedSearchCV and HalvingGridSearchCV. RandomizedSearchCV randomly samples a fixed number of parameter settings from the grid, making it more efficient. HalvingGridSearchCV iteratively reduces the number of candidates, focusing on the most promising ones, thus speeding up the search process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best Parameters: {'tol': 0.1, 'solver': 'saga', 'penalty': 'l2', 'C': 10}\n",
      "Best Cross-Validation Recall: 0.22430469554876162\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'C': [0.01, 0.1, 1, 10, 100], 'penalty': ['l1'], 'solver': ['liblinear', 'saga'], 'tol': [1e-3, 1e-2, 1e-1]},\n",
    "    {'C': [0.01, 0.1, 1, 10, 100], 'penalty': ['l2'], 'solver': ['lbfgs', 'liblinear', 'newton-cg', 'saga'], 'tol': [1e-3, 1e-2, 1e-1]},\n",
    "    {'C': [0.01, 0.1, 1, 10, 100], 'penalty': ['elasticnet'], 'solver': ['saga'], 'tol': [1e-3, 1e-2, 1e-1], 'l1_ratio': [0.1, 0.25, 0.5, 0.75, 0.9]},\n",
    "    {'penalty': [None], 'solver': ['lbfgs', 'newton-cg', 'saga'], 'tol': [1e-3, 1e-2, 1e-1]}\n",
    "]\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_grid,  # Parameter grid remains the same\n",
    "    n_iter=100,  # Number of random parameter combinations to try\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='recall',  # Or other scoring metric of choice\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the random search on training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Cross-Validation Recall:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomizedSearchCV enables testing a subset of parameters from a larger grid. However, since the sampling is random, there is a risk of overlooking optimal parameter combinations. Additionally, it would be beneficial to incorporate different data resampling techniques during cross-validation. However, this can be resource-intensive, especially given the large size of the dataset. To address these challenges, we will implement HalvingGridSearch, which allows for efficient exploration of an extensive parameter grid while accommodating resampling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Halving Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 7\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 7\n",
      "min_resources_: 243\n",
      "max_resources_: 177576\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 828\n",
      "n_resources: 243\n",
      "Fitting 10 folds for each of 828 candidates, totalling 8280 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 276\n",
      "n_resources: 729\n",
      "Fitting 10 folds for each of 276 candidates, totalling 2760 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 92\n",
      "n_resources: 2187\n",
      "Fitting 10 folds for each of 92 candidates, totalling 920 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 31\n",
      "n_resources: 6561\n",
      "Fitting 10 folds for each of 31 candidates, totalling 310 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 11\n",
      "n_resources: 19683\n",
      "Fitting 10 folds for each of 11 candidates, totalling 110 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 4\n",
      "n_resources: 59049\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 2\n",
      "n_resources: 177147\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n",
      "Best Parameters: {'classifier__C': 1, 'classifier__l1_ratio': 0.25, 'classifier__penalty': 'elasticnet', 'classifier__solver': 'saga', 'classifier__tol': 0.001, 'resampler': RandomUnderSampler(random_state=42)}\n",
      "Best Cross-Validation Recall: 0.7655494021263858\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('resampler', None),  # Placeholder for resampling method\n",
    "    ('classifier', LogisticRegression(max_iter=10000, random_state=42))  # Model\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100], 'classifier__penalty': ['l1'], 'classifier__solver': ['liblinear', 'saga'], 'classifier__tol': [1e-3, 1e-2, 1e-1], 'resampler': [RandomOverSampler(random_state=42), RandomUnderSampler(random_state=42), SMOTE(random_state=42), SMOTETomek(random_state=42)]},\n",
    "    {'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100], 'classifier__penalty': ['l2'], 'classifier__solver': ['lbfgs', 'liblinear', 'newton-cg', 'saga'], 'classifier__tol': [1e-3, 1e-2, 1e-1], 'resampler': [RandomOverSampler(random_state=42), RandomUnderSampler(random_state=42), SMOTE(random_state=42), SMOTETomek(random_state=42)]},\n",
    "    {'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100], 'classifier__penalty': ['elasticnet'], 'classifier__solver': ['saga'], 'classifier__tol': [1e-3, 1e-2, 1e-1], 'classifier__l1_ratio': [0.1, 0.25, 0.5, 0.75, 0.9], 'resampler': [RandomOverSampler(random_state=42), RandomUnderSampler(random_state=42), SMOTE(random_state=42), SMOTETomek(random_state=42)]},\n",
    "    {'classifier__penalty': [None], 'classifier__solver': ['lbfgs', 'newton-cg', 'saga'], 'classifier__tol': [1e-3, 1e-2, 1e-1], 'resampler': [RandomOverSampler(random_state=42), RandomUnderSampler(random_state=42), SMOTE(random_state=42), SMOTETomek(random_state=42)]}\n",
    "]\n",
    "\n",
    "# Set up HalvingGridSearchCV\n",
    "halving_grid_search = HalvingGridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=10,  # x-fold cross-validation\n",
    "    scoring='recall',  # Scoring metric\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the halving grid search on training data\n",
    "halving_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", halving_grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Recall:\", halving_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to '../models/logistic_regression/lr_model_sampling_20241124_155301.pkl'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the best model from the halving grid search\n",
    "best_model = halving_grid_search.best_estimator_\n",
    "\n",
    "# Get the current timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save the best model to a file with a timestamp\n",
    "model_filename = f'../models/logistic_regression/lr_model_sampling_{timestamp}.pkl'\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "print(f\"Best model saved to '{model_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 8\n",
      "n_required_iterations: 8\n",
      "n_possible_iterations: 8\n",
      "min_resources_: 81\n",
      "max_resources_: 177576\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 3312\n",
      "n_resources: 81\n",
      "Fitting 5 folds for each of 3312 candidates, totalling 16560 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 1104\n",
      "n_resources: 243\n",
      "Fitting 5 folds for each of 1104 candidates, totalling 5520 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 368\n",
      "n_resources: 729\n",
      "Fitting 5 folds for each of 368 candidates, totalling 1840 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 123\n",
      "n_resources: 2187\n",
      "Fitting 5 folds for each of 123 candidates, totalling 615 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 41\n",
      "n_resources: 6561\n",
      "Fitting 5 folds for each of 41 candidates, totalling 205 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 14\n",
      "n_resources: 19683\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 5\n",
      "n_resources: 59049\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "----------\n",
      "iter: 7\n",
      "n_candidates: 2\n",
      "n_resources: 177147\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Best Parameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'saga', 'classifier__tol': 0.01, 'pca': PCA(), 'resampler': RandomUnderSampler(random_state=42)}\n",
      "Best Cross-Validation Recall: 0.7673088082052849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"resampler\", None),  # Placeholder for resampling method\n",
    "        (\"pca\", None),  # Placeholder for PCA\n",
    "        (\"classifier\", LogisticRegression(max_iter=10000, random_state=42)),  # Model\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"classifier__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        \"classifier__penalty\": [\"l1\"],\n",
    "        \"classifier__solver\": [\"liblinear\", \"saga\"],\n",
    "        \"classifier__tol\": [1e-3, 1e-2, 1e-1],\n",
    "        \"resampler\": [\n",
    "            RandomOverSampler(random_state=42),\n",
    "            RandomUnderSampler(random_state=42),\n",
    "            SMOTE(random_state=42),\n",
    "            SMOTETomek(random_state=42),\n",
    "        ],\n",
    "        \"pca\": [None, PCA(n_components=5), PCA(n_components=10), PCA(n_components=None)],\n",
    "    },\n",
    "    {\n",
    "        \"classifier__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        \"classifier__penalty\": [\"l2\"],\n",
    "        \"classifier__solver\": [\"lbfgs\", \"liblinear\", \"newton-cg\", \"saga\"],\n",
    "        \"classifier__tol\": [1e-3, 1e-2, 1e-1],\n",
    "        \"resampler\": [\n",
    "            RandomOverSampler(random_state=42),\n",
    "            RandomUnderSampler(random_state=42),\n",
    "            SMOTE(random_state=42),\n",
    "            SMOTETomek(random_state=42),\n",
    "        ],\n",
    "        \"pca\": [None, PCA(n_components=5), PCA(n_components=10), PCA(n_components=None)],\n",
    "    },\n",
    "    {\n",
    "        \"classifier__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        \"classifier__penalty\": [\"elasticnet\"],\n",
    "        \"classifier__solver\": [\"saga\"],\n",
    "        \"classifier__tol\": [1e-3, 1e-2, 1e-1],\n",
    "        \"classifier__l1_ratio\": [0.1, 0.25, 0.5, 0.75, 0.9],\n",
    "        \"resampler\": [\n",
    "            RandomOverSampler(random_state=42),\n",
    "            RandomUnderSampler(random_state=42),\n",
    "            SMOTE(random_state=42),\n",
    "            SMOTETomek(random_state=42),\n",
    "        ],\n",
    "        \"pca\": [None, PCA(n_components=5), PCA(n_components=10), PCA(n_components=None)],\n",
    "    },\n",
    "    {\n",
    "        \"classifier__penalty\": [None],\n",
    "        \"classifier__solver\": [\"lbfgs\", \"newton-cg\", \"saga\"],\n",
    "        \"classifier__tol\": [1e-3, 1e-2, 1e-1],\n",
    "        \"resampler\": [\n",
    "            RandomOverSampler(random_state=42),\n",
    "            RandomUnderSampler(random_state=42),\n",
    "            SMOTE(random_state=42),\n",
    "            SMOTETomek(random_state=42),\n",
    "        ],\n",
    "        \"pca\": [None, PCA(n_components=5), PCA(n_components=10), PCA(n_components=None)],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Set up HalvingGridSearchCV\n",
    "halving_grid_search = HalvingGridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring=\"recall\",  # Scoring metric\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1,  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the halving grid search on training data\n",
    "halving_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", halving_grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Recall:\", halving_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to '../models/logistic_regression/lg_model_cv_sampling_pca20241124_154334.pkl'\n"
     ]
    }
   ],
   "source": [
    "# save model to pkl file for later reuse\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the best model from the halving grid search\n",
    "best_model = halving_grid_search.best_estimator_\n",
    "\n",
    "# Get the current timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save the best model to a file with a timestamp\n",
    "model_filename = f'../models/logistic_regression/lg_model_cv_sampling_pca{timestamp}.pkl'\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "print(f\"Best model saved to '{model_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebooke we performed ... \n",
    "\n",
    "The final model selection usese Halving Grid Search with an extensive hyperparameter grid as well as resampling methods, pca, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9436    0.7179    0.8155     21797\n",
      "         1.0     0.3383    0.7707    0.4702      4078\n",
      "\n",
      "    accuracy                         0.7263     25875\n",
      "   macro avg     0.6410    0.7443    0.6428     25875\n",
      "weighted avg     0.8482    0.7263    0.7610     25875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "model_filename = '../models/logistic_regression/lr_model_sampling_20241124_155301.pkl'\n",
    "# Load the model from the pkl file\n",
    "loaded_model = joblib.load(model_filename)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = loaded_model.predict(X_val)\n",
    "\n",
    "# print(y_test_pred)\n",
    "# Evaluate the model's performance on the test set\n",
    "report = classification_report(y_val, y_test_pred, digits=4)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some notes: The “lbfgs” solver is used by default for its robustness. For large datasets the “saga” solver is usually faster.\n",
    "# sklearn crossvalidation (gridsearchcv, ...) use staratified k-fold by default"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
