{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports \n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, PrecisionRecallDisplay\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append(os.path.abspath(\"../scripts\"))\n",
        "from data_loader import DataLoader\n",
        "\n",
        "import joblib\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "data_loader = DataLoader()\n",
        "X_train, y_train  = data_loader.training_data\n",
        "X_val, y_val = data_loader.validation_data\n",
        "X_test, y_test = data_loader.test_data\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}\")\n",
        "print(f\"y_val shape: {y_val.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# logistic regression\n",
        "lr_model_filename = (\n",
        "    \"../models/logistic_regression/lr_model_sampling_20241124_155301.pkl\"\n",
        ")\n",
        "logistic_regression = joblib.load(lr_model_filename)\n",
        "\n",
        "# naive bayes\n",
        "\n",
        "# support vector machine\n",
        "svm_model_filename = \"../models/support_vector_machine/svm_model_halving-grid_full-dataset_accuracy_20241125_230119.pkl\"\n",
        "svm = joblib.load(svm_model_filename)\n",
        "\n",
        "\n",
        "# decision tree\n",
        "\n",
        "# knn\n",
        "knn_filename = \"../models/knn/lr_model_sampling_20241126_150545.pkl\"\n",
        "knn = joblib.load(knn_filename)\n",
        "\n",
        "# Nearest Centroid\n",
        "nc_filename = \"../models/nearest_centroid/lr_model_sampling_20241126_181731.pkl\"\n",
        "nc = joblib.load(nc_filename)\n",
        "\n",
        "# baselines\n",
        "bl_model_filename = \"../models/baseline/bl_model_majority20241126_120730.pkl\"\n",
        "bl_majority = joblib.load(bl_model_filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# all models\n",
        "model_dict = {\n",
        "    \"Logistic Regression\": logistic_regression,\n",
        "    \"Support Vector Machine\": svm,\n",
        "    \"Baseline Majority\": bl_majority,\n",
        "    \"K-Nearest Neighbors\": knn,\n",
        "    \"Nearest Centroid\": nc,\n",
        "    # Add other models here\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model's performance on the test set\n",
        "for model_name, model in model_dict.items():\n",
        "    y_test = y_test\n",
        "    X_test = X_test\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    report = classification_report(y_test, y_test_pred, digits=4)\n",
        "    print(f\"\\n=== Model: {model_name} ===\\n\")\n",
        "    print(\"Classification Report:\\n\", report)\n",
        "    accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
        "    specificity = tn / (tn + fp) # this is basically just the recall of the negative class\n",
        "    print(f\"Specificity: {specificity:.4f}\") \n",
        "    cm = confusion_matrix(y_test, y_test_pred)\n",
        "    print(\"\\nConfusion Matrix:\\n\", cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classification_report(y_test, y_test_pred, digits=4, output_dict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_df = pd.DataFrame(columns=[\n",
        "    'Model', \n",
        "    'Accuracy', \n",
        "    'Overall_Precision', \n",
        "    'Overall_Recall', \n",
        "    'Overall_F1',\n",
        "    'Precision_Negative',\n",
        "    'Precision_Positive',\n",
        "    'Specificity',\n",
        "    'Recall_Negative', \n",
        "    'Recall_Positive', \n",
        "    'F1-Score_Negative', \n",
        "    'F1-Score_Positive', \n",
        "])\n",
        "\n",
        "metrics_list = []  # Temporary list to store rows before creating the final DataFrame\n",
        "\n",
        "for model_name, model in model_dict.items():\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    report = classification_report(y_test, y_test_pred, digits=4, output_dict=True)\n",
        "    accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
        "    specificity = tn / (tn + fp)  # Specificity calculation\n",
        "    \n",
        "    # Metrics for the negative class ('0.0')\n",
        "    precision_negative = report['0.0']['precision']\n",
        "    recall_negative = report['0.0']['recall']\n",
        "    f1_score_negative = report['0.0']['f1-score']\n",
        "    \n",
        "    # Metrics for the positive class ('1.0')\n",
        "    precision_positive = report['1.0']['precision']\n",
        "    recall_positive = report['1.0']['recall']\n",
        "    f1_score_positive = report['1.0']['f1-score']\n",
        "    \n",
        "    # Overall metrics (from 'weighted avg')\n",
        "    overall_precision = report['weighted avg']['precision']\n",
        "    overall_recall = report['weighted avg']['recall']\n",
        "    overall_f1 = report['weighted avg']['f1-score']\n",
        "    \n",
        "    # Append metrics as a dictionary to the list\n",
        "    metrics_list.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Overall_Precision': overall_precision,\n",
        "        'Overall_Recall': overall_recall,\n",
        "        'Overall_F1': overall_f1,\n",
        "        'Precision_Negative': precision_negative,\n",
        "        'Precision_Positive': precision_positive,\n",
        "        'Specificity': specificity,\n",
        "        'Recall_Negative': recall_negative,\n",
        "        'Recall_Positive': recall_positive,\n",
        "        'F1-Score_Negative': f1_score_negative,\n",
        "        'F1-Score_Positive': f1_score_positive,\n",
        "\n",
        "    })\n",
        "\n",
        "# Convert the list of metrics into a DataFrame\n",
        "metrics_df = pd.DataFrame(metrics_list)\n",
        "\n",
        "# Round all numeric values to 4 decimal places\n",
        "metrics_df = metrics_df.round(4)\n",
        "\n",
        "metrics_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Evaluate the model's performance on the test set\n",
        "for model_name, model in model_dict.items():\n",
        "    y_test = y_test\n",
        "    X_test = X_test\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_test_pred)\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
        "    disp.plot(ax=ax, cmap=plt.cm.Blues, values_format='d')\n",
        "    plt.title(f'Confusion Matrix for {model_name}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for model_name, model in model_dict.items():\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    \n",
        "    # Calculate precision, recall, and thresholds\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test, y_test_pred)\n",
        "    average_precision = average_precision_score(y_test, y_test_pred)\n",
        "\n",
        "    # Plot the precision-recall curve\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(recall, precision, marker='.', label=f'{model_name} (AP = {average_precision:.2f})')\n",
        "    plt.xlabel('Recall', fontsize=12)\n",
        "    plt.ylabel('Precision', fontsize=12)\n",
        "    plt.title(f'Precision-Recall Curve for {model_name}', fontsize=14)\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.grid(alpha=0.6, linestyle='--')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for model_name, model in model_dict.items():\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_test_proba = model.predict_proba(X_test)[:, 1]  # Use probabilities for the positive class\n",
        "    else:\n",
        "        # Fallback to decision_function if predict_proba is unavailable\n",
        "        try:\n",
        "            y_test_proba = model.decision_function(X_test)\n",
        "        except:\n",
        "            continue\n",
        "        \n",
        "    # Calculate precision, recall, and thresholds\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test, y_test_proba)\n",
        "    average_precision = average_precision_score(y_test, y_test_proba)\n",
        "\n",
        "    # Plot the precision-recall curve\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(recall, precision, marker='.', label=f'{model_name} (AP = {average_precision:.2f})')\n",
        "    plt.xlabel('Recall', fontsize=12)\n",
        "    plt.ylabel('Precision', fontsize=12)\n",
        "    plt.title(f'Precision-Recall Curve for {model_name}', fontsize=14)\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.grid(alpha=0.6, linestyle='--')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py\n",
        "for model_name, model in model_dict.items():\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    \n",
        "    # Use PrecisionRecallDisplay for visualization\n",
        "    display = PrecisionRecallDisplay.from_predictions(\n",
        "        y_test, y_test_pred,\n",
        "        name=model_name,\n",
        "        plot_chance_level=True  # Adds a baseline for random performance\n",
        "    )\n",
        "\n",
        "    # Customize the plot\n",
        "    display.ax_.set_title(f\"2-Class Precision-Recall Curve for {model_name}\")\n",
        "    display.ax_.grid(alpha=0.6, linestyle='--')  # Add grid\n",
        "    display.ax_.set_xlabel('Recall', fontsize=12)\n",
        "    display.ax_.set_ylabel('Precision', fontsize=12)\n",
        "    display.figure_.set_size_inches(10, 8)  # Adjust figure size\n",
        "    display.ax_.legend(fontsize=12)\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for model_name, model in model_dict.items():\n",
        "    # Use predict_proba to get probabilities for the positive class\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_test_proba = model.predict_proba(X_test)[:, 1]  # Use probabilities for the positive class\n",
        "    else:\n",
        "        # Fallback to decision_function if predict_proba is unavailable\n",
        "        try:\n",
        "            y_test_proba = model.decision_function(X_test)\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    # Use PrecisionRecallDisplay for visualization\n",
        "    display = PrecisionRecallDisplay.from_predictions(\n",
        "        y_test, y_test_proba,  # Use probabilities instead of binary predictions\n",
        "        name=model_name,\n",
        "        plot_chance_level=True  # Adds a baseline for random performance\n",
        "    )\n",
        "\n",
        "    # Customize the plot\n",
        "    display.ax_.set_title(f\"2-Class Precision-Recall Curve for {model_name}\")\n",
        "    display.ax_.grid(alpha=0.6, linestyle='--')  # Add grid\n",
        "    display.ax_.set_xlabel('Recall', fontsize=12)\n",
        "    display.ax_.set_ylabel('Precision', fontsize=12)\n",
        "    display.figure_.set_size_inches(10, 8)  # Adjust figure size\n",
        "    display.ax_.legend(fontsize=12)\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for model_name, model in model_dict.items():\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_test_proba = model.predict_proba(X_test)[:, 1]  # Use probabilities for the positive class\n",
        "    else:\n",
        "        # Fallback to decision_function if predict_proba is unavailable\n",
        "        y_test_proba = model.decision_function(X_test)\n",
        "    \n",
        "    # Calculate ROC curve\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
        "    roc_auc = roc_auc_score(y_test, y_test_proba)\n",
        "\n",
        "    # Plot the ROC curve\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(fpr, tpr, marker='.', label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title(f'ROC Curve for {model_name}', fontsize=14)\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.grid(alpha=0.6, linestyle='--')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
