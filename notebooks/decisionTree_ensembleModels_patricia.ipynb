{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9567238d",
   "metadata": {},
   "source": [
    "# Decision Tree and Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84194252",
   "metadata": {},
   "source": [
    "This notebook deals with the Decision Tree Classifier and various Ensemble Models/ Methods.\n",
    "First, we want to compare the performance of a standard DecisionTreeClassifier to Ensemble Models such as examples for Bagging and Boosting on the preprocessed training data.\n",
    "Secondly, halving grid search is used to perform hyperparameter tuning for each model with included cross-validation, different resampling methods, and furthermore, principle component analysis (PCA) is taken into account. \n",
    "Eventually, precision and recall curves are used for analysis and the final evaluation of the best performing classifier out of the selected Decision Tree and Ensemble Methods is conducted by comparing the different models based on F1 taken as the scoring metric as part of the halving grid search (hyperparameter tuning) for all models.\n",
    "\n",
    "\n",
    "The steps followed in this notebook are:\n",
    "1. **Initial Exploration of Models**: Evaluate the performance of DecisionTreeClassifier and Ensemble Methods such as Bagging and Boosting (with two examples for each) on the preprocessed training data.\n",
    "2. **Hyperparameter Tuning - Each Model**: Use halving grid search with cross validaton, different resampling methods, and PCA (data with reduced dimensionality) to find the optimal hyperparameters for each of the classifiers based on the F1 score.\n",
    "2. **Hyperparameter Tuning - Best Model**: Use halving grid search with cross validaton, different resampling methods, and PCA (data with reduced dimensionality) to find the best model with the optimal hyperparameters based on the F1 score.\n",
    "5. **Precision Recall Analysis**: Plot precision recall curves of the results of the hyperparameter tuning of all the models individually and, finally, the best performing model due to the fact that the scoring metric F1 is composed of the harmonic mean of precision and recall.\n",
    "\n",
    "By the above mentioned steps, we intend to find the best performing classifier with respect to the F1 score out of the selected Decision Tree and Ensemble Models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a106b60",
   "metadata": {},
   "source": [
    "# 1. Initial Exploration of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bba31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make imports and preparations to load the data\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))\n",
    "from data_loader import DataLoader\n",
    "\n",
    "# Load the data\n",
    "data_loader = DataLoader()\n",
    "X_train, y_train = data_loader.training_data\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4514a46c-6420-4ee6-84cc-a3c6f2b23d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for the decision tree model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the decision tree model with default parameters\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = decision_tree.predict(X_train)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = decision_tree.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24786a8e",
   "metadata": {},
   "source": [
    "=> TODO Overfitting because training accuracy almost 100% and validation accuracy only 78%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be05c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba07f455-fe3d-4734-a45b-e6e6172b0519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO is three layers already too much?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Visualize the tree of the model but only show the first three layers (max_depth=3)\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    decision_tree,\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=[\"No Diabetes\", \"Diabetes\"],\n",
    "    max_depth=3 \n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ccd1f6",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b794bf8",
   "metadata": {},
   "source": [
    "### Bagging (Example: Bagging with Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9718aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for the bagging classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the base estimator\n",
    "estimator = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Initialize the Bagging classifier with the base estimator\n",
    "bagging_decision_tree = BaggingClassifier(estimator=estimator, n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "bagging_decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = bagging_decision_tree.predict(X_train)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = bagging_decision_tree.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cf8b22",
   "metadata": {},
   "source": [
    "=> still overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde10b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO is three layers already too much?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Visualize the first tree of the model but only show the first three layers (max_depth=3)\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    bagging_decision_tree.estimators_[0],\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=[\"No Diabetes\", \"Diabetes\"],\n",
    "    max_depth=3 \n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce5c2a0",
   "metadata": {},
   "source": [
    "### Bagging (Example: Random Forest Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7f64ee-9608-4532-bfbd-c1ac86cc5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#TODO Adjust max_depth to? \n",
    "#TODO random_state = 0 soll üblich sein für reproducability, habe aber auch 42 gelesen als arbitrary number?\n",
    "#TODO is it okay to set zero_division to 1?\n",
    "\n",
    "# Initialize the random forest ensemble model with default parameters\n",
    "bagging_random_forest = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "bagging_random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = bagging_random_forest.predict(X_train)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = bagging_random_forest.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aeb64e",
   "metadata": {},
   "source": [
    "=> Still overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974cc51e-28e2-45e0-900d-102106e9e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO is three layers already too much?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Visualize the first tree of the model but only show the first three layers (max_depth=3)\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    bagging_random_forest.estimators_[0],  \n",
    "    feature_names=X_train.columns,\n",
    "    class_names=[\"No Diabetes\", \"Diabetes\"],\n",
    "    max_depth=3 \n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4491637",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba671a21",
   "metadata": {},
   "source": [
    "### Boosting (Example: Adaptive Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d9a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Initialize the AdaBoost ensemble model (with a decision tree as the default base classifier and other default parameters)\n",
    "adaptive_boosting = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "adaptive_boosting.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = adaptive_boosting.predict(X_train)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = adaptive_boosting.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e74d394",
   "metadata": {},
   "source": [
    "=> No overfitting, finally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f5f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO only one layer => decision stump\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Visualize the first tree of the model (with this model only one split and layer occurs)\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    adaptive_boosting.estimators_[0],  \n",
    "    feature_names=X_train.columns,\n",
    "    class_names=[\"No Diabetes\", \"Diabetes\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1437c8",
   "metadata": {},
   "source": [
    "=> only one split, only one layer occurs (this is called \"Stump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ee999f",
   "metadata": {},
   "source": [
    "### Boosting (Example: Extreme Gradient Boosting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1517b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize the XGBoost ensemble model (with a decision tree as the default base classifier and other default parameters)\n",
    "extreme_gradient_boosting = XGBClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "extreme_gradient_boosting.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = extreme_gradient_boosting.predict(X_train)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = extreme_gradient_boosting.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01096e2",
   "metadata": {},
   "source": [
    "=> No overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6616583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO is three layers already too much?\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn import tree\n",
    "\n",
    "# # Visualize the first tree of the model but only show the first three layers (max_depth=3)\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# tree.plot_tree(\n",
    "#     extreme_gradient_boosting.get_booster().get_dump()[0],  \n",
    "#     feature_names=X_train.columns,\n",
    "#     class_names=[\"No Diabetes\", \"Diabetes\"],\n",
    "#     max_depth=3\n",
    "# )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee99ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO => is not from the sklearn library, but from the xgboost library\n",
    "# therefore need to find different way to visualize the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce642c0",
   "metadata": {},
   "source": [
    "### Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ea960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test random undersampling\n",
    "X_train_undersampling_random, y_train_undersampling_random = data_loader.training_data_undersampling_random\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data\n",
    "\n",
    "print(f\"X_train_undersampling_random shape: {X_train_undersampling_random.shape}\")\n",
    "print(f\"y_train_undersampling_random shape: {y_train_undersampling_random.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e70f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for the decision tree model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the decision tree model with default parameters\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "decision_tree.fit(X_train_undersampling_random, y_train_undersampling_random)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred_undersampling_random = decision_tree.predict(X_train_undersampling_random)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train_undersampling_random, y_train_pred_undersampling_random)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = decision_tree.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28902c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO is three layers already too much?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Visualize the tree of the model but only show the first three layers (max_depth=3)\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    decision_tree,\n",
    "    feature_names=X_train_undersampling_random.columns,\n",
    "    class_names=[\"No Diabetes\", \"Diabetes\"],\n",
    "    max_depth=3 \n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dab1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for the bagging classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the base estimator\n",
    "estimator = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Initialize the Bagging classifier with the base estimator\n",
    "bagging_decision_tree = BaggingClassifier(estimator=estimator, n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "bagging_decision_tree.fit(X_train_undersampling_random, y_train_undersampling_random)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred_undersampling_random = bagging_decision_tree.predict(X_train_undersampling_random)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train_undersampling_random, y_train_pred_undersampling_random)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = bagging_decision_tree.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ada6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    " #TODO is three layers already too much?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Visualize the first tree of the model but only show the first three layers (max_depth=3)\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    bagging_decision_tree.estimators_[0],\n",
    "    feature_names=X_train_undersampling_random.columns,\n",
    "    class_names=[\"No Diabetes\", \"Diabetes\"],\n",
    "    max_depth=3 \n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d35524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Initialize the AdaBoost ensemble model (with a decision tree as the default base classifier and other default parameters)\n",
    "adaptive_boosting = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "adaptive_boosting.fit(X_train_undersampling_random, y_train_undersampling_random)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred_undersampling_random = adaptive_boosting.predict(X_train_undersampling_random)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train_undersampling_random, y_train_pred_undersampling_random)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = adaptive_boosting.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23334c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO only one layer => decision stump\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Visualize the first tree of the model (with this model only one split and layer occurs)\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    adaptive_boosting.estimators_[0],  \n",
    "    feature_names=X_train_undersampling_random.columns,\n",
    "    class_names=[\"No Diabetes\", \"Diabetes\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9406c2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize the XGBoost ensemble model (with a decision tree as the default base classifier and other default parameters)\n",
    "extreme_gradient_boosting = XGBClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "extreme_gradient_boosting.fit(X_train_undersampling_random, y_train_undersampling_random)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred_undersampling_random = extreme_gradient_boosting.predict(X_train_undersampling_random)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train_undersampling_random, y_train_pred_undersampling_random)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = extreme_gradient_boosting.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ddcf6b",
   "metadata": {},
   "source": [
    "### Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b043cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test random oversampling\n",
    "X_train_oversampling_random, y_train_oversampling_random = data_loader.training_data_oversampling_random\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data\n",
    "\n",
    "print(f\"X_train_oversampling_random shape: {X_train_oversampling_random.shape}\")\n",
    "print(f\"y_train_oversampling_random shape: {y_train_oversampling_random.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00f4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for the decision tree model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the decision tree model with default parameters\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "decision_tree.fit(X_train_oversampling_random, y_train_oversampling_random)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred_oversampling_random = decision_tree.predict(X_train_oversampling_random)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train_oversampling_random, y_train_pred_oversampling_random)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = decision_tree.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92799beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO is three layers already too much?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Visualize the tree of the model but only show the first three layers (max_depth=3)\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    decision_tree,\n",
    "    feature_names=X_train_oversampling_random.columns,\n",
    "    class_names=[\"No Diabetes\", \"Diabetes\"],\n",
    "    max_depth=3 \n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for the bagging classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the base estimator\n",
    "estimator = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Initialize the Bagging classifier with the base estimator\n",
    "bagging_decision_tree = BaggingClassifier(estimator=estimator, n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "bagging_decision_tree.fit(X_train_oversampling_random, y_train_oversampling_random)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred_oversampling_random = bagging_decision_tree.predict(X_train_oversampling_random)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train_oversampling_random, y_train_pred_oversampling_random)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = bagging_decision_tree.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c5ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    " #TODO is three layers already too much?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Visualize the first tree of the model but only show the first three layers (max_depth=3)\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    bagging_decision_tree.estimators_[0],\n",
    "    feature_names=X_train_oversampling_random.columns,\n",
    "    class_names=[\"No Diabetes\", \"Diabetes\"],\n",
    "    max_depth=3 \n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e312e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Initialize the AdaBoost ensemble model (with a decision tree as the default base classifier and other default parameters)\n",
    "adaptive_boosting = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "adaptive_boosting.fit(X_train_oversampling_random, y_train_oversampling_random)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred_oversampling_random = adaptive_boosting.predict(X_train_oversampling_random)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train_oversampling_random, y_train_pred_oversampling_random)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = adaptive_boosting.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9471665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO only one layer => decision stump\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Visualize the first tree of the model (with this model only one split and layer occurs)\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    adaptive_boosting.estimators_[0],  \n",
    "    feature_names=X_train_oversampling_random.columns,\n",
    "    class_names=[\"No Diabetes\", \"Diabetes\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c9a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize the XGBoost ensemble model (with a decision tree as the default base classifier and other default parameters)\n",
    "extreme_gradient_boosting = XGBClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "extreme_gradient_boosting.fit(X_train_oversampling_random, y_train_oversampling_random)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred_oversampling_random = extreme_gradient_boosting.predict(X_train_oversampling_random)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train_oversampling_random, y_train_pred_oversampling_random)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = extreme_gradient_boosting.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3962512b",
   "metadata": {},
   "source": [
    "### SMOTE Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1748a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test smote oversampling\n",
    "X_train_oversampling_smote, y_train_oversampling_smote = data_loader.training_data_oversampling_smote\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data\n",
    "\n",
    "print(f\"X_train_oversampling_smote shape: {X_train_oversampling_smote.shape}\")\n",
    "print(f\"y_train_oversampling_smote shape: {y_train_oversampling_smote.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1660ce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for the decision tree model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the decision tree model with default parameters\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "decision_tree.fit(X_train_oversampling_smote, y_train_oversampling_smote)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred_oversampling_smote = decision_tree.predict(X_train_oversampling_smote)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train_oversampling_smote, y_train_pred_oversampling_smote)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = decision_tree.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO is three layers already too much?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Visualize the tree of the model but only show the first three layers (max_depth=3)\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    decision_tree,\n",
    "    feature_names=X_train_oversampling_smote.columns,\n",
    "    class_names=[\"No Diabetes\", \"Diabetes\"],\n",
    "    max_depth=3 \n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for the bagging classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the base estimator\n",
    "estimator = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Initialize the Bagging classifier with the base estimator\n",
    "bagging_decision_tree = BaggingClassifier(estimator=estimator, n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "bagging_decision_tree.fit(X_train_oversampling_smote, y_train_oversampling_smote)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred_oversampling_smote = bagging_decision_tree.predict(X_train_oversampling_smote)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train_oversampling_smote, y_train_pred_oversampling_smote)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = bagging_decision_tree.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c3c030",
   "metadata": {},
   "outputs": [],
   "source": [
    " #TODO is three layers already too much?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Visualize the first tree of the model but only show the first three layers (max_depth=3)\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    bagging_decision_tree.estimators_[0],\n",
    "    feature_names=X_train_oversampling_smote.columns,\n",
    "    class_names=[\"No Diabetes\", \"Diabetes\"],\n",
    "    max_depth=3 \n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc3d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Initialize the AdaBoost ensemble model (with a decision tree as the default base classifier and other default parameters)\n",
    "adaptive_boosting = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "adaptive_boosting.fit(X_train_oversampling_smote, y_train_oversampling_smote)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred_oversampling_smote = adaptive_boosting.predict(X_train_oversampling_smote)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train_oversampling_smote, y_train_pred_oversampling_smote)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = adaptive_boosting.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO only one layer => decision stump\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Visualize the first tree of the model (with this model only one split and layer occurs)\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    adaptive_boosting.estimators_[0],  \n",
    "    feature_names=X_train_oversampling_smote.columns,\n",
    "    class_names=[\"No Diabetes\", \"Diabetes\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdaa831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize the XGBoost ensemble model (with a decision tree as the default base classifier and other default parameters)\n",
    "extreme_gradient_boosting = XGBClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "extreme_gradient_boosting.fit(X_train_oversampling_smote, y_train_oversampling_smote)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred_oversampling_smote = extreme_gradient_boosting.predict(X_train_oversampling_smote)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train_oversampling_smote, y_train_pred_oversampling_smote)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = extreme_gradient_boosting.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59793d24",
   "metadata": {},
   "source": [
    "### SMOTE Tomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608a4dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_oversampling_smote_tomek, y_train_oversampling_smote_tomek = data_loader.training_data_resampling_smote_tomek\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data\n",
    "\n",
    "print(f\"X_train_oversampling_smote shape: {X_train_oversampling_smote_tomek.shape}\")\n",
    "print(f\"y_train_oversampling_smote shape: {y_train_oversampling_smote_tomek.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be9b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for the decision tree model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the decision tree model with default parameters\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "decision_tree.fit(X_train_oversampling_smote_tomek, y_train_oversampling_smote_tomek)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred_oversampling_smote_tomek = decision_tree.predict(X_train_oversampling_smote_tomek)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train_oversampling_smote_tomek, y_train_pred_oversampling_smote_tomek)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = decision_tree.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4c26cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO is three layers already too much?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Visualize the tree of the model but only show the first three layers (max_depth=3)\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    decision_tree,\n",
    "    feature_names=X_train_oversampling_smote_tomek.columns,\n",
    "    class_names=[\"No Diabetes\", \"Diabetes\"],\n",
    "    max_depth=3 \n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for the bagging classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the base estimator\n",
    "estimator = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Initialize the Bagging classifier with the base estimator\n",
    "bagging_decision_tree = BaggingClassifier(estimator=estimator, n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "bagging_decision_tree.fit(X_train_oversampling_smote_tomek, y_train_oversampling_smote_tomek)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred_oversampling_smote_tomek = bagging_decision_tree.predict(X_train_oversampling_smote_tomek)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train_oversampling_smote_tomek, y_train_pred_oversampling_smote_tomek)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = bagging_decision_tree.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6980d",
   "metadata": {},
   "outputs": [],
   "source": [
    " #TODO is three layers already too much?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Visualize the first tree of the model but only show the first three layers (max_depth=3)\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    bagging_decision_tree.estimators_[0],\n",
    "    feature_names=X_train_oversampling_smote_tomek.columns,\n",
    "    class_names=[\"No Diabetes\", \"Diabetes\"],\n",
    "    max_depth=3 \n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e463d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Initialize the AdaBoost ensemble model (with a decision tree as the default base classifier and other default parameters)\n",
    "adaptive_boosting = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "adaptive_boosting.fit(X_train_oversampling_smote_tomek, y_train_oversampling_smote_tomek)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred_oversampling_smote_tomek = adaptive_boosting.predict(X_train_oversampling_smote_tomek)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train_oversampling_smote_tomek, y_train_pred_oversampling_smote_tomek)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = adaptive_boosting.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8236e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO only one layer => decision stump\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Visualize the first tree of the model (with this model only one split and layer occurs)\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    adaptive_boosting.estimators_[0],  \n",
    "    feature_names=X_train_oversampling_smote_tomek.columns,\n",
    "    class_names=[\"No Diabetes\", \"Diabetes\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ec4679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize the XGBoost ensemble model (with a decision tree as the default base classifier and other default parameters)\n",
    "extreme_gradient_boosting = XGBClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "extreme_gradient_boosting.fit(X_train_oversampling_smote_tomek, y_train_oversampling_smote_tomek)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred_oversampling_smote_tomek = extreme_gradient_boosting.predict(X_train_oversampling_smote_tomek)\n",
    "\n",
    "# Evaluate the model's performance on the training dataset\n",
    "accuracy_train = accuracy_score(y_train_oversampling_smote_tomek, y_train_pred_oversampling_smote_tomek)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = extreme_gradient_boosting.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset\n",
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "print(\"Training Accuracy\", accuracy_train)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb98144",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning (with Cross-Validation and PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8132ba4",
   "metadata": {},
   "source": [
    "### Halving Grid Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f380d9a4",
   "metadata": {},
   "source": [
    "#### Halving Grid Search for Decision Tree Classifier (with Cross-Validation and PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d9fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"resampler\", None),  # Placeholder for resampling method\n",
    "        (\"pca\", None),  # Placeholder for PCA\n",
    "        ('classifier', None)  # Placeholder for classifier\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "# Source of Hyperparameters for Decision Tree & ChatGPT (ranges for large dataset):  https://ken-hoffman.medium.com/decision-tree-hyperparameters-explained-49158ee1268e\n",
    "    {\n",
    "    'classifier': [DecisionTreeClassifier()],\n",
    "    'classifier__max_depth': [1, 2, 5, 10, 50, 100, None], \n",
    "    'classifier__max_leaf_nodes': [1, 2, 5, 10, 50, 100, 500, None], \n",
    "    'classifier__max_features': ['auto', 'sqrt', 'log2', None],  \n",
    "    'classifier__min_samples_split': [2, 5, 10, 50, 100], \n",
    "    'classifier__min_samples_leaf': [1, 2, 5, 10, 20, 50], \n",
    "    'classifier__criterion': ['gini', 'entropy', 'log_loss'],  \n",
    "    'classifier__splitter': ['best', 'random'],  \n",
    "    'resampler': [\n",
    "        None,\n",
    "        RandomOverSampler(random_state=42),\n",
    "        RandomUnderSampler(random_state=42),\n",
    "        SMOTE(random_state=42),\n",
    "        SMOTETomek(random_state=42)\n",
    "    ],  \n",
    "    \"pca\": [None, PCA(n_components=5), PCA(n_components=10), PCA(n_components=None)\n",
    "    ]  # PCA options for dimensionality reduction.\n",
    "}\n",
    "]\n",
    "\n",
    "# Set up HalvingGridSearchCV\n",
    "halving_grid_search = HalvingGridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    scoring=\"f1\",  # Scoring metric\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1,  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the halving grid search on training data\n",
    "halving_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", halving_grid_search.best_params_)\n",
    "print(\"Best Cross-Validation F1 Score:\", halving_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e8c7d",
   "metadata": {},
   "source": [
    "#### Halving Grid Search for Bagging Classifier (with Cross-Validation and PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb5c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"resampler\", None),  # Placeholder for resampling method\n",
    "        (\"pca\", None),  # Placeholder for PCA\n",
    "        ('classifier', None)  # Placeholder for classifier\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "\n",
    "# Source of Hyperparameters for Bagging Classifier & ChatGPT (ranges for large dataset): https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "   {\n",
    "    'classifier': [BaggingClassifier(estimator=DecisionTreeClassifier())],\n",
    "    'classifier__n_estimators': [10, 50, 100, 200], \n",
    "    'classifier__max_samples': [0.5, 0.7, 1.0],  \n",
    "    'classifier__max_features': [0.25, 0.5, 0.7, 1.0],  \n",
    "    'classifier__bootstrap': [True, False],  \n",
    "    'classifier__oob_score': [True, False],  \n",
    "    'classifier__warm_start': [True, False],  \n",
    "    'classifier__n_jobs': [None,-1],  \n",
    "    'classifier__random_state': [42], \n",
    "    'classifier__verbose': [0, 1], \n",
    "    'resampler': [\n",
    "        None,\n",
    "        RandomOverSampler(random_state=42),\n",
    "        RandomUnderSampler(random_state=42),\n",
    "        SMOTE(random_state=42),\n",
    "        SMOTETomek(random_state=42)\n",
    "    ]\n",
    "   }\n",
    "]\n",
    "\n",
    "# Set up HalvingGridSearchCV\n",
    "halving_grid_search = HalvingGridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    scoring=\"f1\",  # Scoring metric\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1,  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the halving grid search on training data\n",
    "halving_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", halving_grid_search.best_params_)\n",
    "print(\"Best Cross-Validation F1 Score:\", halving_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d43fc7",
   "metadata": {},
   "source": [
    "#### Halving Grid Search for Random Forest Classifier (with Cross-Validation and PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2495455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"resampler\", None),  # Placeholder for resampling method\n",
    "        (\"pca\", None),  # Placeholder for PCA\n",
    "        ('classifier', None)  # Placeholder for classifier\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "\n",
    "    # Source of Hyperparameters for Random Forest Classifier adjusted ranges with ChatGPT\n",
    "    {\n",
    "    'classifier': [RandomForestClassifier()],\n",
    "    'classifier__max_depth': [1, 2, 5, 10, 50, 100, None], \n",
    "    'classifier__max_features': ['sqrt', 'log2', 0.5, 0.7, 1.0],  \n",
    "    'classifier__min_samples_split': [2, 3, 5, 10, 50, 100], \n",
    "    'classifier__bootstrap': [True, False],  \n",
    "    'classifier__criterion': ['gini', 'entropy', 'log_loss'], \n",
    "    'resampler': [\n",
    "        None,\n",
    "        RandomOverSampler(random_state=42),\n",
    "        RandomUnderSampler(random_state=42),\n",
    "        SMOTE(random_state=42),\n",
    "        SMOTETomek(random_state=42)\n",
    "    ],  \n",
    "    \"pca\": [\n",
    "        None, \n",
    "        PCA(n_components=5), \n",
    "        PCA(n_components=10), \n",
    "        PCA(n_components=None)\n",
    "    ]  \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Set up HalvingGridSearchCV\n",
    "halving_grid_search = HalvingGridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    scoring=\"f1\",  # Scoring metric\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1,  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the halving grid search on training data\n",
    "halving_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", halving_grid_search.best_params_)\n",
    "print(\"Best Cross-Validation F1 Score:\", halving_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e57b853",
   "metadata": {},
   "source": [
    "#### Halving Grid Search for Adaptive Boosting Classifier (with Cross-Validation and PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b9f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"resampler\", None),  # Placeholder for resampling method\n",
    "        (\"pca\", None),  # Placeholder for PCA\n",
    "        ('classifier', None)  # Placeholder for classifier\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "    # Source of Hyperparameters for Adaptive Boosting Classifier adjusted ranges with ChatGPT\n",
    "    {\n",
    "    'classifier': [AdaBoostClassifier()],\n",
    "    'classifier__n_estimators': [10, 50, 100, 500], \n",
    "    'classifier__learning_rate': [0.001, 0.01, 0.1, 1, 10],  \n",
    "    'classifier__algorithm': ['SAMME'],\n",
    "    'classifier__random_state': [42],  \n",
    "    'resampler': [\n",
    "        None,\n",
    "        RandomOverSampler(random_state=42),\n",
    "        RandomUnderSampler(random_state=42),\n",
    "        SMOTE(random_state=42),\n",
    "        SMOTETomek(random_state=42)\n",
    "    ],  \n",
    "    \"pca\": [\n",
    "        None, \n",
    "        PCA(n_components=5), \n",
    "        PCA(n_components=10), \n",
    "        PCA(n_components=None)\n",
    "    ]  \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Set up HalvingGridSearchCV\n",
    "halving_grid_search = HalvingGridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    scoring=\"f1\",  # Scoring metric\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1,  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the halving grid search on training data\n",
    "halving_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", halving_grid_search.best_params_)\n",
    "print(\"Best Cross-Validation F1 Score:\", halving_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36234bad",
   "metadata": {},
   "source": [
    "#### Halving Grid Search for Extreme Gradient Boosting Classifier (with Cross-Validation and PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a030dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"resampler\", None),  # Placeholder for resampling method\n",
    "        (\"pca\", None),  # Placeholder for PCA\n",
    "        ('classifier', None)  # Placeholder for classifier\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "# Source of Hyperparameters for XGBoost & ChatGPT: https://medium.com/@amitsinghrajput_92567/understanding-hyperparameters-in-decision-trees-xgboost-and-lightgbm-7b64cfed77f0\n",
    "    {\n",
    "        'classifier': [XGBClassifier()],\n",
    "        'classifier__max_depth': [1, 2, 5, 10, 50, 100, None], \n",
    "        'classifier__learning_rate': [0.001, 0.01, 0.1, 1, 10],  \n",
    "        'classifier__min_child_weight': [1, 3, 5, 10, 50], \n",
    "        'classifier__gamma': [0, 0.1, 0.5, 1, 5, 10],\n",
    "        'classifier__subsample': [0, 0.1, 0.3, 0.5, 0.7, 1],\n",
    "        'classifier__colsample_bytree': [0, 0.1, 0.3, 0.5, 0.7, 1],\n",
    "        'resampler': [\n",
    "        RandomOverSampler(random_state=42),\n",
    "        RandomUnderSampler(random_state=42),\n",
    "        SMOTE(random_state=42),\n",
    "        SMOTETomek(random_state=42)\n",
    "    ],  \n",
    "    \"pca\": [\n",
    "        None, \n",
    "        PCA(n_components=5), \n",
    "        PCA(n_components=10), \n",
    "        PCA(n_components=None)\n",
    "    ]  \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Set up HalvingGridSearchCV\n",
    "halving_grid_search = HalvingGridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    scoring=\"f1\",  # Scoring metric\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1,  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the halving grid search on training data\n",
    "halving_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", halving_grid_search.best_params_)\n",
    "print(\"Best Cross-Validation F1 Score:\", halving_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0216a6b",
   "metadata": {},
   "source": [
    "### Halving Grid Search for All Classifiers (with Cross-Validation and PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff2148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"resampler\", None),  # Placeholder for resampling method\n",
    "        (\"pca\", None),  # Placeholder for PCA\n",
    "        ('classifier', None)  # Placeholder for classifier\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "# Source of Hyperparameters for Decision Tree & ChatGPT (ranges for large dataset):  https://ken-hoffman.medium.com/decision-tree-hyperparameters-explained-49158ee1268e\n",
    "    {\n",
    "    'classifier': [DecisionTreeClassifier()],\n",
    "    'classifier__max_depth': [1, 2, 5, 10, 50, 100, None], \n",
    "    'classifier__max_leaf_nodes': [1, 2, 5, 10, 50, 100, 500, None], \n",
    "    'classifier__max_features': ['auto', 'sqrt', 'log2', None],  \n",
    "    'classifier__min_samples_split': [2, 5, 10, 50, 100], \n",
    "    'classifier__min_samples_leaf': [1, 2, 5, 10, 20, 50], \n",
    "    'classifier__criterion': ['gini', 'entropy', 'log_loss'],  \n",
    "    'classifier__splitter': ['best', 'random'],  \n",
    "    'resampler': [\n",
    "        None,\n",
    "        RandomOverSampler(random_state=42),\n",
    "        RandomUnderSampler(random_state=42),\n",
    "        SMOTE(random_state=42),\n",
    "        SMOTETomek(random_state=42)\n",
    "    ],  \n",
    "    \"pca\": [None, PCA(n_components=5), PCA(n_components=10), PCA(n_components=None)\n",
    "    ]  # PCA options for dimensionality reduction.\n",
    "},\n",
    "# # Source of Hyperparameters for Bagging Classifier & ChatGPT (ranges for large dataset): https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "#    {\n",
    "#     'classifier': [BaggingClassifier(estimator=DecisionTreeClassifier())],\n",
    "#     'classifier__n_estimators': [10, 50, 100, 200], \n",
    "#     'classifier__max_samples': [0.5, 0.7, 1.0],  \n",
    "#     'classifier__max_features': [0.25, 0.5, 0.7, 1.0],  \n",
    "#     'classifier__bootstrap': [True, False],  \n",
    "#     'classifier__oob_score': [True, False],  \n",
    "#     'classifier__warm_start': [True, False],  \n",
    "#     'classifier__n_jobs': [None,-1],  \n",
    "#     'classifier__random_state': [42], \n",
    "#     'classifier__verbose': [0, 1], \n",
    "#     'resampler': [\n",
    "#         None,\n",
    "#         RandomOverSampler(random_state=42),\n",
    "#         RandomUnderSampler(random_state=42),\n",
    "#         SMOTE(random_state=42),\n",
    "#         SMOTETomek(random_state=42)\n",
    "#     ],  \n",
    "#     'pca': [\n",
    "#         None, \n",
    "#         PCA(n_components=5), \n",
    "#         PCA(n_components=10), \n",
    "#         PCA(n_components=None)\n",
    "#     ] \n",
    "#     },\n",
    "    # Source of Hyperparameters for Random Forest Classifier adjusted ranges with ChatGPT\n",
    "    {\n",
    "    'classifier': [RandomForestClassifier()],\n",
    "    'classifier__max_depth': [1, 2, 5, 10, 50, 100, None], \n",
    "    'classifier__max_features': ['sqrt', 'log2', 0.5, 0.7, 1.0],  \n",
    "    'classifier__min_samples_split': [2, 3, 5, 10, 50, 100], \n",
    "    'classifier__bootstrap': [True, False],  \n",
    "    'classifier__criterion': ['gini', 'entropy', 'log_loss'], \n",
    "    'resampler': [\n",
    "        None,\n",
    "        RandomOverSampler(random_state=42),\n",
    "        RandomUnderSampler(random_state=42),\n",
    "        SMOTE(random_state=42),\n",
    "        SMOTETomek(random_state=42)\n",
    "    ],  \n",
    "    \"pca\": [\n",
    "        None, \n",
    "        PCA(n_components=5), \n",
    "        PCA(n_components=10), \n",
    "        PCA(n_components=None)\n",
    "    ]  \n",
    "    },\n",
    "    # Source of Hyperparameters for Adaptive Boosting Classifier adjusted ranges with ChatGPT\n",
    "    {\n",
    "    'classifier': [AdaBoostClassifier()],\n",
    "    'classifier__n_estimators': [10, 50, 100, 500], \n",
    "    'classifier__learning_rate': [0.001, 0.01, 0.1, 1, 10],  \n",
    "    'classifier__algorithm': ['SAMME'],\n",
    "    'classifier__random_state': [42],  \n",
    "    'resampler': [\n",
    "        None,\n",
    "        RandomOverSampler(random_state=42),\n",
    "        RandomUnderSampler(random_state=42),\n",
    "        SMOTE(random_state=42),\n",
    "        SMOTETomek(random_state=42)\n",
    "    ],  \n",
    "    \"pca\": [\n",
    "        None, \n",
    "        PCA(n_components=5), \n",
    "        PCA(n_components=10), \n",
    "        PCA(n_components=None)\n",
    "    ]  \n",
    "    },\n",
    "# Source of Hyperparameters for XGBoost & ChatGPT: https://medium.com/@amitsinghrajput_92567/understanding-hyperparameters-in-decision-trees-xgboost-and-lightgbm-7b64cfed77f0\n",
    "    {\n",
    "        'classifier': [XGBClassifier()],\n",
    "        'classifier__max_depth': [1, 2, 5, 10, 50, 100, None], \n",
    "        'classifier__learning_rate': [0.001, 0.01, 0.1, 1, 10],  \n",
    "        'classifier__min_child_weight': [1, 3, 5, 10, 50], \n",
    "        'classifier__gamma': [0, 0.1, 0.5, 1, 5, 10],\n",
    "        'classifier__subsample': [0, 0.1, 0.3, 0.5, 0.7, 1],\n",
    "        'classifier__colsample_bytree': [0, 0.1, 0.3, 0.5, 0.7, 1],\n",
    "        'resampler': [\n",
    "        RandomOverSampler(random_state=42),\n",
    "        RandomUnderSampler(random_state=42),\n",
    "        SMOTE(random_state=42),\n",
    "        SMOTETomek(random_state=42)\n",
    "    ],  \n",
    "    \"pca\": [\n",
    "        None, \n",
    "        PCA(n_components=5), \n",
    "        PCA(n_components=10), \n",
    "        PCA(n_components=None)\n",
    "    ]  \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Set up HalvingGridSearchCV\n",
    "halving_grid_search = HalvingGridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    scoring=\"f1\",  # Scoring metric\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1,  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the halving grid search on training data\n",
    "halving_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Model w/ Best Parameters:\", halving_grid_search.best_params_)\n",
    "print(\"Best Cross-Validation F1 Score:\", halving_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf349ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Precision Recall Curves for Positive Class of Best Performing Classifier (with respect to F1 Score)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, classification_report\n",
    "\n",
    "# Assuming grid_search is the result of your hyperparameter tuning\n",
    "best_clf = halving_grid_search.best_estimator_\n",
    "\n",
    "# Fit the best classifier on the training data\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the positive class of target variable\n",
    "y_scores = best_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, label='Precision-Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve for Positive Class')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate F1 score for each threshold\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Find the threshold that gives the best F1 score\n",
    "best_threshold = thresholds[f1_scores.argmax()]\n",
    "\n",
    "# Print the best F1 score and the corresponding threshold\n",
    "print(f'Best F1 Score: {f1_scores.max():.2f}')\n",
    "print(f'Best Threshold: {best_threshold:.2f}')\n",
    "\n",
    "# Classification report for the best threshold\n",
    "y_pred_best_threshold = (y_scores >= best_threshold).astype(int)\n",
    "print(classification_report(y_test, y_pred_best_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0b4829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Precision Recall Curves for Negative Class of Best Performing Classifier (with respect to F1 Score)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, classification_report\n",
    "\n",
    "# Assuming grid_search is the result of your hyperparameter tuning\n",
    "best_clf = halving_grid_search.best_estimator_\n",
    "\n",
    "# Fit the best classifier on the training data\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the negative class\n",
    "y_scores_negative = best_clf.predict_proba(X_test)[:, 0]\n",
    "\n",
    "# Calculate precision and recall for the negative class\n",
    "precision_neg, recall_neg, thresholds_neg = precision_recall_curve(1 - y_test, y_scores_negative)\n",
    "\n",
    "# Plot Precision-Recall curve for the negative class of target variable \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall_neg, precision_neg, label='Precision-Recall curve (Negative Class)')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve for Negative Class')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate F1 score for each threshold\n",
    "f1_scores_neg = 2 * (precision_neg * recall_neg) / (precision_neg + recall_neg)\n",
    "\n",
    "# Find the threshold that gives the best F1 score\n",
    "best_threshold_neg = thresholds_neg[f1_scores_neg.argmax()]\n",
    "\n",
    "# Print the best F1 score and the corresponding threshold\n",
    "print(f'Best F1 Score (Negative Class): {f1_scores_neg.max():.2f}')\n",
    "print(f'Best Threshold (Negative Class): {best_threshold_neg:.2f}')\n",
    "\n",
    "# Classification report for the best threshold\n",
    "y_pred_best_threshold_neg = (y_scores_negative >= best_threshold_neg).astype(int)\n",
    "print(classification_report(1 - y_test, y_pred_best_threshold_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5adda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to pkl file for later reuse\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the best model from the halving grid search\n",
    "best_model = halving_grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150dca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = best_model.predict(X_val)\n",
    "\n",
    "# print(y_test_pred)\n",
    "# Evaluate the model's performance on the test set\n",
    "report = classification_report(y_val, y_test_pred, digits=4)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce373c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# from datetime import datetime\n",
    "\n",
    "# # Get the best model from the halving grid search\n",
    "# best_model = halving_grid_search.best_estimator_\n",
    "\n",
    "# # Get the current timestamp\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# # Save the best model to a file with a timestamp\n",
    "# model_filename = f'../models/decision_tree_ensemble_methods/lr_model_sampling_{timestamp}.pkl'\n",
    "# joblib.dump(best_model, model_filename)\n",
    "\n",
    "# print(f\"Best model saved to '{model_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e8467",
   "metadata": {},
   "source": [
    "## Further Possible Exploration: Base Learners for Voting and Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992184a-5e96-46e1-9d9a-fb4343a57f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# #TODO How many neighbours n= ?\n",
    "# #TODO Do I need probability=True for soft voting?\n",
    "# # TODO Need the best n for k-NN? => I chose 5 for now\n",
    "\n",
    "# #Initialize base classifiers \n",
    "# base_classifiers = {\n",
    "#     \"decision tree\": DecisionTreeClassifier(),\n",
    "#     \"random forest\": RandomForestClassifier(),\n",
    "#     \"logisitc regression\": LogisticRegression(),\n",
    "#     \"naive bayes\":GaussianNB(), \n",
    "#     \"support vector machines\": SVC(probability=True), \n",
    "#     \"k-NN with n = 5\": KNeighborsClassifier(n_neighbors=5)\n",
    "# }\n",
    "\n",
    "# # TODO => should I rather use recall?\n",
    "# # Implement function to display accuracy as performance metric for different classifiers\n",
    "\n",
    "# def evaluate_classifier(e_name, e, X_train, y_train, X_val, y_val):\n",
    "#     # Train the model on the preprocessed training data\n",
    "#     e.fit(X_train, y_train)\n",
    "\n",
    "#     # Make predictions on the training set\n",
    "#     y_train_pred = e.predict(X_train)\n",
    "\n",
    "#     # Evaluate the model's performance on the training dataset\n",
    "#     accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "#     # Make predictions on the validation set\n",
    "#     y_val_pred = e.predict(X_val)\n",
    "\n",
    "#     # Evaluate the model's performance on the validation dataset\n",
    "#     report = classification_report(y_val, y_val_pred, digits=4)\n",
    "\n",
    "#     print(f'Training Accuracy of {e_name}: {accuracy_train}')\n",
    "#     print(f'Classification Report of {e_name}:\\n\", {report}')\n",
    "\n",
    "\n",
    "#     # y_val_pred = e.fit(X_train, y_train).predict(X_val)\n",
    "#     # acc = accuracy_score(y_val, y_pred)\n",
    "#     # print(f'{e_name}: ACC={acc:.2f}')\n",
    "\n",
    "# # Evaluate the base classifiers on the preprocessed training and validation data\n",
    "# for e_name, e in base_classifiers.items():\n",
    "#     evaluate_classifier(e_name, e, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bfbf9b-f5e6-47f0-8f6d-e7c0edc2d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO n_estimator = ? by hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3409d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO for Voting => look if classifiers are independent, no correlation between predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d651ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# import numpy as np\n",
    "\n",
    "# # Get predictions from the classifiers\n",
    "# predictions_knn = estimators['k-NN'].predict(X_test)\n",
    "# predictions_ncc = estimators['NCC'].predict(X_test)\n",
    "\n",
    "# # Calculate the correlation between the predictions\n",
    "# correlation = np.corrcoef(predictions_knn, predictions_ncc)[0, 1]\n",
    "# print(f'Correlation between k-NN and NCC predictions: {correlation:.2f}')\n",
    "\n",
    "# # Check if the classifiers are dependent\n",
    "# if correlation > 0.5:\n",
    "#     print(\"The classifiers are likely dependent.\")\n",
    "# else:\n",
    "#     print(\"The classifiers are likely independent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea98ae3",
   "metadata": {},
   "source": [
    "### Voting (Example: with all base learners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2561d1b4-3987-47f0-99cd-367171f1b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Use voting ensemble method as classifier\n",
    "# #TODO random_states = ? und weights = ? voting soft=?\n",
    "# voting = VotingClassifier(\n",
    "#     (\"dt\", DecisionTreeClassifier(),\n",
    "#      \"rf\", RandomForestClassifier(),\n",
    "#      \"lr\", LogisticRegression(),\n",
    "#      \"nb\",GaussianNB(), \n",
    "#      \"svm\", SVC(probability=True), # SVM with probabilities for soft voting\n",
    "#      \"knn_5\" KNeighborsClassifier(n_neighbors=5)                           \n",
    "#      )  \n",
    "# )\n",
    "\n",
    "# #TODO Complete voting\n",
    "# #TODO Evaluate the classifiers' accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d7441",
   "metadata": {},
   "source": [
    "### Stacking (Example: with all base learners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1936218e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
