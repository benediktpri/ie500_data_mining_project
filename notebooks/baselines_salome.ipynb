{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))\n",
    "from data_loader import DataLoader\n",
    "\n",
    "data_loader = DataLoader()\n",
    "X_train, y_train = data_loader.training_data\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Baselines are on the unsampled, whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Majority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"most frequent\": Always picks the most common class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority class for our dataset is \"no diabetes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.842439293598234\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      1.00      0.91     21371\n",
      "         1.0       0.00      0.00      0.00      3997\n",
      "\n",
      "    accuracy                           0.84     25368\n",
      "   macro avg       0.42      0.50      0.46     25368\n",
      "weighted avg       0.71      0.84      0.77     25368\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Initialize the support vector machine model\n",
    "baseline_majority = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "baseline_majority.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred_majority = baseline_majority.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy_majority = accuracy_score(y_val, y_val_pred_majority)\n",
    "report_majority = classification_report(y_val, y_val_pred_majority)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy_majority}\")\n",
    "print(\"Classification Report:\\n\", report_majority)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another standard baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"stratified\": Makes random guesses based on the original class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is in our case: \n",
    "Predicts no-diabetes with a probability of 218334/253680 = 0.86\n",
    "and diabetes with a probability of 35346/253680 = 0.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7362425102491328\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.85      0.84     21371\n",
      "         1.0       0.15      0.14      0.15      3997\n",
      "\n",
      "    accuracy                           0.74     25368\n",
      "   macro avg       0.50      0.50      0.50     25368\n",
      "weighted avg       0.73      0.74      0.73     25368\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the support vector machine model\n",
    "baseline_stratified = DummyClassifier(strategy=\"stratified\")\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "baseline_stratified.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred_stratified = baseline_stratified.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy_stratified = accuracy_score(y_val, y_val_pred_stratified)\n",
    "report_stratified = classification_report(y_val, y_val_pred_stratified)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy_stratified}\")\n",
    "print(\"Classification Report:\\n\", report_stratified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Distribution baseline has a lower accuracy than the Majority Class baseline. But for the positive class (i.e., diabetes) precision, recall and f1-score are significantly higher, because they are 0 for Majority Class. Correspondingly, recall and f1-score of the negative class (i.e., no-diabetes) decreased from the Majority Class to Distribution baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Use feature with highest correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature that has the highest correlation with our target (diabetes) is GenHealth (with 0.29). This correlation is still far below 0.5. Thus, we can be sure that it is no false predictor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick the feature having the highest correlation with our target (diabetes). \n",
    "Then, we calculate the mean of the target variable for each the ordinal values for that selected feature.\n",
    "Where we find the highest difference/gap between these target-means, is where we put our decision boundary.\n",
    "For this feature's values below that decision boundary, diabetes is predicted.\n",
    "For this feature's values above that decision boundary, no diabetes is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most correlated feature: GenHlth\n",
      "Means (grouped by class): {1.0: 0.031711541489529205, 2.0: 0.08568687030225491, 3.0: 0.20092068122833273, 4.0: 0.342883087400681, 5.0: 0.4081849274678618}\n",
      "Indices of the largest gap: (3.0, 4.0)\n",
      "\n",
      "Validation Accuracy: 0.7940483091787439\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.87      0.88     21797\n",
      "         1.0       0.36      0.40      0.38      4078\n",
      "\n",
      "    accuracy                           0.79     25875\n",
      "   macro avg       0.62      0.63      0.63     25875\n",
      "weighted avg       0.80      0.79      0.80     25875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate correlation of each feature in X_train with y_train\n",
    "correlations = X_train.apply(lambda x: x.corr(y_train))\n",
    "# Find the feature with the highest correlation\n",
    "most_correlated_feature = correlations.idxmax()\n",
    "print(f\"Most correlated feature: {most_correlated_feature}\")\n",
    "    \n",
    "# Get unique values of this feature\n",
    "unique_values = X_train[most_correlated_feature].dropna().unique()\n",
    "means = {}\n",
    "\n",
    "# Combine X_train and y_train into a single DataFrame\n",
    "combined_df = pd.concat([X_train, y_train.rename('y_train')], axis=1)\n",
    "# Rename negative labels from 0 to -1\n",
    "\n",
    "# Group by the unique values in the specified column and calculate the mean of y_train\n",
    "# Get correlation with GenHlth for each of its values\n",
    "for unique_val in unique_values:\n",
    "    #correlations = combined_df[genhlth_dummies.columns].corrwith(combined_df['Diabetes_binary'])\n",
    "    subset = combined_df[combined_df[most_correlated_feature] == unique_val]\n",
    "    mu = subset['y_train'].mean()\n",
    "    means[unique_val] = mu\n",
    "\n",
    "# Sort dictionary of correlations increasingly by its key\n",
    "means = dict(sorted(means.items()))\n",
    "keys = list(means.keys())\n",
    "values = list(means.values())\n",
    "print(f\"Means (grouped by class): {means}\")\n",
    "\n",
    "# Find the largest gap\n",
    "largest_gap = 0\n",
    "indices_largest_gap = (None, None)\n",
    "for i in range(len(values) - 1):\n",
    "    gap = values[i+1] - values[i]\n",
    "    if gap > largest_gap:\n",
    "        largest_gap = gap\n",
    "        indices_largest_gap = (keys[i], keys[i+1])\n",
    "\n",
    "# Switch two values of largest gap \n",
    "# so that the first corresponds to no diabetes and the second to diabetes\n",
    "if means[indices_largest_gap[0]] > means[indices_largest_gap[1]]:\n",
    "    helper_switch = indices_largest_gap[0]\n",
    "    indices_largest_gap[0] = indices_largest_gap[1]\n",
    "    indices_largest_gap[1] = helper_switch\n",
    "print(f\"Indices of the largest gap: {indices_largest_gap}\\n\")\n",
    "\n",
    "# Make prediction on validation set\n",
    "y_val_pred_one_feature = pd.DataFrame({\n",
    "    'Target': X_val[most_correlated_feature].apply(lambda x: 1 if x >= indices_largest_gap[1] else 0)\n",
    "})\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy_one_feature = accuracy_score(y_val, y_val_pred_one_feature)\n",
    "report_one_feature = classification_report(y_val, y_val_pred_one_feature)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy_one_feature}\")\n",
    "print(\"Classification Report:\\n\", report_one_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: KNN on First component of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this too good for a baseline? Should we use another (more dummy) classifier than K-NN for this first PCA component?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a conventional baseline, but since it is not overly complicated, it is suitable as a baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8167\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.95      0.90     21797\n",
      "         1.0       0.26      0.09      0.13      4078\n",
      "\n",
      "    accuracy                           0.82     25875\n",
      "   macro avg       0.55      0.52      0.51     25875\n",
      "weighted avg       0.76      0.82      0.78     25875\n",
      "\n",
      "Explained variance by the first component: 0.4719632380321297\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Apply PCA to reduce dimensions (you can choose the number of components, here it's set to 2 for visualization)\n",
    "pca = PCA(n_components=1)  # just the first component (it's a baseline)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_val_pca = pca.transform(X_val)\n",
    "\n",
    "# Initialize the K-NN classifier (with k=5, you can adjust the number of neighbors)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn.predict(X_val_pca)\n",
    "\n",
    "# Measure performance\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "# Output results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Explained variance by the first component\n",
    "# = variance of that principal component / total variance\n",
    "# = variance of that principal component / sum of variances of all individual principal components\n",
    "explained_variance_ratio_first = pca.explained_variance_ratio_[0]\n",
    "print(f\"Explained variance by the first component: {explained_variance_ratio_first}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO in report: argue why it makes sense -> Explain/use/include the theory of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we do not plot the explained variance of different numbers of principal components, because using any more than one component seems to be too much for a baseline that should be kept simple."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
