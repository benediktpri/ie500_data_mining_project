{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))\n",
    "from data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader()\n",
    "X_train, y_train = data_loader.training_data\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (177576, 21)\n",
      "y_train shape: (177576,)\n",
      "X_val shape: (25875, 21)\n",
      "y_val shape: (25875,)\n",
      "X_test shape: (50229, 21)\n",
      "y_test shape: (50229,)\n",
      "Number of negative samples in training set: 149592\n",
      "Number of positive samples in training set: 27984\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "# (data == value_to_count) creates a boolean Series where True corresponds to occurrences of the specific value.\n",
    "# .sum() counts the True values (since True is equivalent to 1 in Python).\n",
    "print(f\"Number of negative samples in training set: {(y_train == 0.0).sum()}\")\n",
    "print(f\"Number of positive samples in training set: {(y_train == 1.0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Save model to pkl file for later reuse\n",
    "def save_model (model, model_name):\n",
    "    # Get the current timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Save the best model to a file with a timestamp\n",
    "    model_filename = f'../models/support_vector_machine/svm_model_{model_name}_{timestamp}.pkl'\n",
    "    joblib.dump(model, model_filename)\n",
    "\n",
    "    print(f\"Initial model saved to '{model_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we just try out the Support Vector Machine to get an initial feeling how it performs and to have something to improve upon in the following sections using Resampling, Hyperparameter Tuning, and PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the support vector machine model\n",
    "model_initial = SVC(C=1.0, kernel='rbf', verbose=True)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "model_initial.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = model_initial.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically the exact same result as for the majority classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it has such a long runtime (), we save it. Of course, we hope that we do not have to reuse it because we improve it in the following cells. But better safe than sorry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to pkl file for later reuse\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save the best model to a file with a timestamp\n",
    "model_filename = f'../models/support_vector_machine/svm_model_initial_{timestamp}.pkl'\n",
    "joblib.dump(model_initial, model_filename)\n",
    "\n",
    "print(f\"Initial model saved to '{model_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_undersampling_random shape: (55968, 21)\n",
      "y_train_undersampling_random shape: (55968,)\n",
      "X_val shape: (25875, 21)\n",
      "y_val shape: (25875,)\n",
      "X_test shape: (50229, 21)\n",
      "y_test shape: (50229,)\n",
      "Number of negative samples in undersampled training set: 27984\n",
      "Number of positive samples in undersampled training set: 27984\n"
     ]
    }
   ],
   "source": [
    "# test random undersampling\n",
    "X_train_undersampling_random, y_train_undersampling_random = data_loader.training_data_undersampling_random\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data\n",
    "\n",
    "print(f\"X_train_undersampling_random shape: {X_train_undersampling_random.shape}\")\n",
    "print(f\"y_train_undersampling_random shape: {y_train_undersampling_random.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# (data == value_to_count) creates a boolean Series where True corresponds to occurrences of the specific value.\n",
    "# .sum() counts the True values (since True is equivalent to 1 in Python).\n",
    "print(f\"Number of negative samples in undersampled training set: {(y_train_undersampling_random == 0.0).sum()}\")\n",
    "print(f\"Number of positive samples in undersampled training set: {(y_train_undersampling_random == 1.0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the undersampling, we have as many positive as negative examples in our training data. This more balanced training set might be useful to improve the recall on the positive class, i.e., diabetes (previously, the minority class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By random undersampling, we reduced the original training dataset containing 177567 examples to 55968 examples, which is approximately a third of the original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "# Initialize the support vector machine model\n",
    "model_undersampling = SVC(C=1.0, kernel='rbf', verbose=True)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "model_undersampling.fit(X_train_undersampling_random, y_train_undersampling_random)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred_undersampling = model_undersampling.predict(X_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6922125603864734\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.67      0.79     21797\n",
      "         1.0       0.32      0.81      0.45      4078\n",
      "\n",
      "    accuracy                           0.69     25875\n",
      "   macro avg       0.63      0.74      0.62     25875\n",
      "weighted avg       0.85      0.69      0.73     25875\n",
      "\n",
      "[16269 16310]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance\n",
    "accuracy_undersampling = accuracy_score(y_val, y_val_pred_undersampling)\n",
    "report_undersampling = classification_report(y_val, y_val_pred_undersampling)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy_undersampling}\")\n",
    "print(\"Classification Report:\\n\", report_undersampling)\n",
    "\n",
    "print(f\"Number of support vectors for each class: {model_undersampling.n_support_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduction of data points to a third drastically improves the runtime from over 40 minutes (TODO!) to 3 minutes (in our setting). This makes sense since the training time of a Support Vector Machine generally has a cubic runtime O(n^3), i.e., it grows cubically with the number n of training examples (source: https://stackoverflow.com/questions/18165213/how-much-time-does-it-take-to-train-a-svm-classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM on undersampled data yields a better recall for the positive class than SVM on the full training data. But as expected for higher recall, but the overall accuracy suffers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model saved to '../models/support_vector_machine/svm_model_undersampling_20241125_105832.pkl'\n"
     ]
    }
   ],
   "source": [
    "save_model(model_undersampling, \"undersampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper for theory why Undersampling is so good for SVMs: https://www.sciencedirect.com/science/article/pii/S1474667016429952"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: one or two other undersampling methods so that I can compare?\n",
    "# ideas:\n",
    "# - slighter/softer undersampling (i.e., majority class has just 2x more examples) -> slightly more examples\n",
    "# - undersampling the SMOTE tomek\n",
    "# - more ideas: https://imbalanced-learn.org/stable/references/under_sampling.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing random oversampling for SVMs would make training time even longer. For comparison, the original training dataset has 177567 samples and our undersampled dataset 63964 samples (i.e., approximately a third of the original). Our oversampled dataset (the random version and also the SMOTE version) has 299184 samples. Thus, is not performed for SVMs here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing SMOTE oversampling for SVMs would make training time even longer. For comparison, the original training dataset has 177567 samples and our undersampled dataset 63964 samples (i.e., approximately a third of the original). Our oversampled dataset (the random version and also the SMOTE version) has 299184 samples. Thus, is not performed for SVMs here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE Tomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_oversampling_smote shape: (298548, 21)\n",
      "y_train_oversampling_smote shape: (298548,)\n",
      "X_val shape: (25875, 21)\n",
      "y_val shape: (25875,)\n",
      "X_test shape: (50229, 21)\n",
      "y_test shape: (50229,)\n",
      "Number of negative samples in SMOTE Tomek training set: 149274\n",
      "Number of positive samples in SMOTE Tomek training set: 149274\n"
     ]
    }
   ],
   "source": [
    "X_train_oversampling_smote_tomek, y_train_oversampling_smote_tomek = data_loader.training_data_resampling_smote_tomek\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data\n",
    "\n",
    "print(f\"X_train_oversampling_smote shape: {X_train_oversampling_smote_tomek.shape}\")\n",
    "print(f\"y_train_oversampling_smote shape: {y_train_oversampling_smote_tomek.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"Number of negative samples in SMOTE Tomek training set: {(y_train_oversampling_smote_tomek == 0.0).sum()}\")\n",
    "print(f\"Number of positive samples in SMOTE Tomek training set: {(y_train_oversampling_smote_tomek == 1.0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the SMOTE Tomek sampling, we have as many positive as negative examples in our training data, as after the Random Undersampling. Here again, we hope that this more balanced training set might be useful to improve the recall on the positive class (i.e., diabetes) (previously, the minority class) compared with the original dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By SMOTE Tomek sampling, we increased the original training dataset containing 177567 examples to 298548 examples, which is roughly 1.5 times the size of the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "# Initialize the support vector machine model\n",
    "model_smote_tomek = SVC(C=1.0, kernel='rbf', verbose=True)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "model_smote_tomek.fit(X_train_oversampling_smote_tomek, y_train_oversampling_smote_tomek)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred_smote_tomek = model_smote_tomek.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy_smote_tomek = accuracy_score(y_val, y_val_pred_smote_tomek)\n",
    "report_smote_tomek = classification_report(y_val, y_val_pred_smote_tomek)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy_smote_tomek}\")\n",
    "print(\"Classification Report:\\n\", report_smote_tomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model_smote_tomek, \"smote_tomek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compare inparticular original, random undersampling, and SMOTE Tomek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tune the following hyperparameters: the type of kernel (e.g., linear, rbf, polynomial, sigmoid), the regularization parameter (C), and kernel-specific parameters like gamma for the RBF kernel and the degree for polynomial kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to time complexity, hyperparameter tuning is only feasible on undersampled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Parameters: {'C': 100, 'kernel': 'linear'}\n",
      "Best Cross-Validation Accuracy: 0.5680699561747314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning with Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, CondensedNearestNeighbour\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "    'kernel': ['rbf', 'linear', 'poly'],  # Kernel type\n",
    "    'gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001],  # Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’\n",
    "    'degree': [2, 3, 4, 5], # Degree of the polynomial kernel function (‘poly’)\n",
    "    # 'penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization type  # not possible for SVM\n",
    "    # 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga', 'sag'],  # Optimization algorithm  # not possible for SVM\n",
    "    'tol': [1e-3, 1e-2, 1e-1]  # Tolerance for stopping criteria  # TODO if time allows\n",
    "    # 'resampler': [RandomUnderSampler(random_state=42), TomekLinks(), CondensedNearestNeighbour()]\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = SVC(max_iter=1000, random_state=42)\n",
    "\n",
    "# Choose the search method - here we're using GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='recall',  # we want to optimize recall\n",
    "    n_jobs=-1,  # Use all available processors\n",
    "    verbose=1  # To see the progress\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_undersampling_random, y_train_undersampling_random)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.717675812046673\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.75      0.82     21371\n",
      "         1.0       0.29      0.54      0.38      3997\n",
      "\n",
      "    accuracy                           0.72     25368\n",
      "   macro avg       0.59      0.65      0.60     25368\n",
      "weighted avg       0.80      0.72      0.75     25368\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the best model\n",
    "best_model_grid = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = best_model_grid.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best Parameters: {'resampler': RandomUnderSampler(random_state=42), 'classifier__tol': 0.001, 'classifier__kernel': 'linear', 'classifier__C': 0.1}\n",
      "Best Cross-Validation Recall: 0.7448948130718961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning with Random Search\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, CondensedNearestNeighbour\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('resampler', None),  # Placeholder for resampling method\n",
    "    ('classifier', SVC(max_iter=10000, random_state=42))  # Model\n",
    "])\n",
    "\n",
    "param_grid_toy = {\n",
    "        'classifier__kernel': ['linear'],  # Kernel type\n",
    "        'classifier__C': [0.1],  # Regularization strength\n",
    "        # 'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization type  # not possible for SVM\n",
    "        # 'classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga', 'sag'],  # Optimization algorithm  # not possible for SVM\n",
    "        'classifier__tol': [1e-3],  # Tolerance for stopping criteria  # TODO if time allows\n",
    "        'resampler': [RandomUnderSampler(random_state=42)] # many other options: TomekLinks(), CondensedNearestNeighbour(), ...\n",
    "    }\n",
    "param_grid = [\n",
    "    {\n",
    "        'classifier__kernel': ['linear'],  # Kernel type\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "        # 'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization type  # not possible for SVM\n",
    "        # 'classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga', 'sag'],  # Optimization algorithm  # not possible for SVM\n",
    "        'classifier__tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria  # TODO if time allows\n",
    "        'resampler': [RandomUnderSampler(random_state=42), RandomUnderSampler(random_state=42, sampling_strategy=0.5)] # many other options: TomekLinks(), CondensedNearestNeighbour(), ...\n",
    "    }, \n",
    "    {\n",
    "        'classifier__kernel': ['poly'],  # Kernel type\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "        'classifier__degree': [2, 3, 4, 5], # Degree of the polynomial kernel function (only for ‘poly’)\n",
    "        'classifier__gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001],  # Kernel coefficient (only for ‘rbf’, ‘poly’ and ‘sigmoid’)\n",
    "        'classifier__coef0': [-1.0, 0.0, 1.0],  # Independent term in kernel function (only for ‘poly’ and ‘sigmoid’)\n",
    "        # 'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization type  # not possible for SVM\n",
    "        # 'classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga', 'sag'],  # Optimization algorithm  # not possible for SVM\n",
    "        'classifier__tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria  # TODO if time allows\n",
    "        'resampler': [RandomUnderSampler(random_state=42), RandomUnderSampler(random_state=42, sampling_strategy=0.5)] # many other options: TomekLinks(), CondensedNearestNeighbour(), ...\n",
    "    }, \n",
    "    {\n",
    "        'classifier__kernel': ['rbf'],  # Kernel type\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "        'classifier__gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001],  # Kernel coefficient  # Kernel coefficient (only for ‘rbf’, ‘poly’ and ‘sigmoid’)\n",
    "        # 'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization type  # not possible for SVM\n",
    "        # 'classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga', 'sag'],  # Optimization algorithm  # not possible for SVM\n",
    "        'classifier__tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria  # TODO if time allows\n",
    "        'resampler': [RandomUnderSampler(random_state=42), RandomUnderSampler(random_state=42, sampling_strategy=0.5)] # many other options: TomekLinks(), CondensedNearestNeighbour(), ...\n",
    "    }\n",
    "]\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_grid_toy,  # Parameter grid remains the same  # TODO\n",
    "    n_iter=1,#100,  # Number of random parameter combinations to try  # TODO\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='recall',  # we want to optimize recall\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the random search on training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Cross-Validation Recall:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best Parameters: {'tol': 0.001, 'kernel': 'poly', 'gamma': 0.001, 'degree': 4, 'C': 100}\n",
      "Best Cross-Validation Recall: 0.9838478070108783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning with Random Search\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, CondensedNearestNeighbour\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('resampler', None),  # Placeholder for resampling method\n",
    "    ('classifier', LogisticRegression(max_iter=10000, random_state=42))  # Model\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "    'kernel': ['rbf', 'linear', 'poly'],  # Kernel type\n",
    "    'gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001],  # Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’\n",
    "    'degree': [2, 3, 4, 5], # Degree of the polynomial kernel function (‘poly’)\n",
    "    # 'penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization type  # not possible for SVM\n",
    "    # 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga', 'sag'],  # Optimization algorithm  # not possible for SVM\n",
    "    'tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria  # TODO if time allows\n",
    "    'resampler': [RandomUnderSampler(random_state=42), TomekLinks(), CondensedNearestNeighbour()]\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = SVC(random_state=42, max_iter=1000)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_grid,  # Parameter grid remains the same\n",
    "    n_iter=100,  # Number of random parameter combinations to try\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='recall',  # we want to optimize recall\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the random search on training data\n",
    "random_search.fit(X_train_undersampling_random, y_train_undersampling_random)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Cross-Validation Recall:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.16154589371980677\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.01      0.02     21797\n",
      "         1.0       0.15      0.97      0.27      4078\n",
      "\n",
      "    accuracy                           0.16     25875\n",
      "   macro avg       0.40      0.49      0.14     25875\n",
      "weighted avg       0.56      0.16      0.06     25875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model_random = random_search.best_estimator_\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = best_model_random.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model of randomized search yields a worse accuracy than the best model of grid search. Also the f1-score is worse. Thus, it seems to make sense to go with the model identified by grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Halving Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.experimental import enable_halving_search_cv\n",
    "# from sklearn.model_selection import HalvingGridSearchCV\n",
    "# from imblearn.pipeline import Pipeline\n",
    "# from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# from imblearn.combine import SMOTETomek\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('resampler', None),  # Placeholder for resampling method\n",
    "#     ('classifier', SVC(max_iter=10000, random_state=42))  # Model\n",
    "# ])\n",
    "\n",
    "# param_grid = [\n",
    "#     {'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100], \n",
    "#      'classifier__penalty': ['l1'], \n",
    "#      'classifier__solver': ['liblinear', 'saga'], \n",
    "#      'classifier__tol': [1e-3, 1e-2, 1e-1], \n",
    "#      'resampler': [RandomOverSampler(random_state=42), RandomUnderSampler(random_state=42), SMOTE(random_state=42), SMOTETomek(random_state=42)]\n",
    "#      },\n",
    "#     {'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100], \n",
    "#      'classifier__penalty': ['l2'], \n",
    "#      'classifier__solver': ['lbfgs', 'liblinear', 'newton-cg', 'saga'], \n",
    "#      'classifier__tol': [1e-3, 1e-2, 1e-1], \n",
    "#      'resampler': [RandomOverSampler(random_state=42), RandomUnderSampler(random_state=42), SMOTE(random_state=42), SMOTETomek(random_state=42)]\n",
    "#      },\n",
    "#     {'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100], \n",
    "#      'classifier__penalty': ['elasticnet'], \n",
    "#      'classifier__solver': ['saga'], \n",
    "#      'classifier__tol': [1e-3, 1e-2, 1e-1], \n",
    "#      'classifier__l1_ratio': [0.1, 0.25, 0.5, 0.75, 0.9], \n",
    "#      'resampler': [RandomOverSampler(random_state=42), RandomUnderSampler(random_state=42), SMOTE(random_state=42), SMOTETomek(random_state=42)]\n",
    "#      },\n",
    "#     {'classifier__penalty': [None], \n",
    "#      'classifier__solver': ['lbfgs', 'newton-cg', 'saga'], \n",
    "#      'classifier__tol': [1e-3, 1e-2, 1e-1], \n",
    "#      'resampler': [RandomOverSampler(random_state=42), RandomUnderSampler(random_state=42), SMOTE(random_state=42), SMOTETomek(random_state=42)]\n",
    "#      }\n",
    "# ]\n",
    "\n",
    "# # Set up HalvingGridSearchCV\n",
    "# halving_grid_search = HalvingGridSearchCV(\n",
    "#     estimator=pipeline,\n",
    "#     param_grid=param_grid,\n",
    "#     cv=10,  # x-fold cross-validation\n",
    "#     scoring='recall',  # we want to optimize recall\n",
    "#     n_jobs=-1,  # Use all processors\n",
    "#     verbose=1  # To track progress\n",
    "# )\n",
    "\n",
    "# # Fit the halving grid search on training data\n",
    "# halving_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best parameters and score\n",
    "# print(\"Best Parameters:\", halving_grid_search.best_params_)\n",
    "# print(\"Best Cross-Validation Recall:\", halving_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 8\n",
      "n_required_iterations: 8\n",
      "n_possible_iterations: 8\n",
      "min_resources_: 81\n",
      "max_resources_: 177576\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 2844\n",
      "n_resources: 81\n",
      "Fitting 5 folds for each of 2844 candidates, totalling 14220 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 948\n",
      "n_resources: 243\n",
      "Fitting 5 folds for each of 948 candidates, totalling 4740 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 316\n",
      "n_resources: 729\n",
      "Fitting 5 folds for each of 316 candidates, totalling 1580 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 106\n",
      "n_resources: 2187\n",
      "Fitting 5 folds for each of 106 candidates, totalling 530 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 36\n",
      "n_resources: 6561\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 12\n",
      "n_resources: 19683\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 4\n",
      "n_resources: 59049\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "----------\n",
      "iter: 7\n",
      "n_candidates: 2\n",
      "n_resources: 177147\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Best Parameters: {'classifier__C': 0.01, 'classifier__coef0': 1.0, 'classifier__degree': 5, 'classifier__gamma': 'scale', 'classifier__kernel': 'poly', 'classifier__tol': 0.01, 'resampler': RandomUnderSampler(random_state=42)}\n",
      "Best Cross-Validation F1-Score: 0.2720925658802119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning with Halving Grid Search\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, CondensedNearestNeighbour\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('resampler', None),  # Placeholder for resampling method\n",
    "    ('classifier', SVC(max_iter=10000, random_state=42))  # Model\n",
    "])\n",
    "\n",
    "param_grid_toy = {\n",
    "    'classifier__kernel': ['linear'],  # Kernel type\n",
    "    'classifier__C': [0.1],  # Regularization strength\n",
    "    # 'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization type  # not possible for SVM\n",
    "    # 'classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga', 'sag'],  # Optimization algorithm  # not possible for SVM\n",
    "    'classifier__tol': [1e-3],  # Tolerance for stopping criteria  # TODO if time allows\n",
    "    'resampler': [RandomUnderSampler(random_state=42)] # many other options: TomekLinks(), CondensedNearestNeighbour(), ...\n",
    "}\n",
    "param_grid_small = {\n",
    "    'classifier__kernel': ['linear'],  # Kernel type\n",
    "    'classifier__C': [0.01, 0.1, 1, 10],  # Regularization strength\n",
    "    'resampler': [RandomUnderSampler(random_state=42), RandomUnderSampler(random_state=42, sampling_strategy=0.5)] # many other options: TomekLinks(), CondensedNearestNeighbour(), ...\n",
    "}\n",
    "param_grid = [\n",
    "    {\n",
    "        'classifier__kernel': ['linear'],  # Kernel type\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "        'classifier__tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria  # TODO if time allows\n",
    "        'resampler': [RandomUnderSampler(random_state=42), RandomUnderSampler(random_state=42, sampling_strategy=0.5)] # many other options: TomekLinks(), CondensedNearestNeighbour(), ...\n",
    "    }, \n",
    "    {\n",
    "        'classifier__kernel': ['poly'],  # Kernel type\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "        'classifier__degree': [2, 3, 4, 5], # Degree of the polynomial kernel function (only for ‘poly’)\n",
    "        'classifier__gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001],  # Kernel coefficient (only for ‘rbf’, ‘poly’ and ‘sigmoid’)\n",
    "        'classifier__coef0': [-1.0, 0.0, 1.0],  # Independent term in kernel function (only for ‘poly’ and ‘sigmoid’)\n",
    "        'classifier__tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria  # TODO if time allows\n",
    "        'resampler': [RandomUnderSampler(random_state=42), RandomUnderSampler(random_state=42, sampling_strategy=0.5)] # many other options: TomekLinks(), CondensedNearestNeighbour(), ...\n",
    "    }, \n",
    "    {\n",
    "        'classifier__kernel': ['rbf'],  # Kernel type\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "        'classifier__gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001],  # Kernel coefficient  # Kernel coefficient (only for ‘rbf’, ‘poly’ and ‘sigmoid’)\n",
    "        'classifier__tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria  # TODO if time allows\n",
    "        'resampler': [RandomUnderSampler(random_state=42), RandomUnderSampler(random_state=42, sampling_strategy=0.5)] # many other options: TomekLinks(), CondensedNearestNeighbour(), ...\n",
    "    }\n",
    "]\n",
    "\n",
    "# Set up HalvingGridSearchCV\n",
    "halving_grid_search = HalvingGridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,  # Parameter grid remains the same  \n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='f1',  # we want to optimize recall  # TODO decide with team\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the random search on training data\n",
    "halving_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", halving_grid_search.best_params_)\n",
    "print(\"Best Cross-Validation F1-Score:\", halving_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.00      0.00     21797\n",
      "         1.0       0.16      1.00      0.27      4078\n",
      "\n",
      "    accuracy                           0.16     25875\n",
      "   macro avg       0.44      0.50      0.14     25875\n",
      "weighted avg       0.64      0.16      0.05     25875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_halving_GS = classification_report(y_val, halving_grid_search.predict(X_val))\n",
    "print(\"Classification Report:\\n\", report_halving_GS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model saved to '../models/support_vector_machine/svm_model_halving-grid_small_20241125_184350.pkl'\n"
     ]
    }
   ],
   "source": [
    "best_model_halving_grid = halving_grid_search.best_estimator_\n",
    "save_model(best_model_halving_grid, \"halving-grid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy as objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with scoring='accuracy' and see whether the results are more balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 9\n",
      "n_required_iterations: 9\n",
      "n_possible_iterations: 9\n",
      "min_resources_: 27\n",
      "max_resources_: 177576\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 9954\n",
      "n_resources: 27\n",
      "Fitting 5 folds for each of 9954 candidates, totalling 49770 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "14220 fits failed out of a total of 49770.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2844 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 329, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 265, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1057, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 112, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\", line 389, in _fit_resample\n",
      "    nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 835, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples_fit, but n_neighbors = 6, n_samples_fit = 3, n_samples = 3\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2844 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 329, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 265, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1057, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 112, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\", line 389, in _fit_resample\n",
      "    nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 835, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples_fit, but n_neighbors = 6, n_samples_fit = 1, n_samples = 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1422 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 329, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 265, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1057, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 112, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\", line 389, in _fit_resample\n",
      "    nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 835, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples_fit, but n_neighbors = 6, n_samples_fit = 2, n_samples = 2\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2844 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 329, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 265, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1057, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 112, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\combine\\_smote_tomek.py\", line 157, in _fit_resample\n",
      "    X_res, y_res = self.smote_.fit_resample(X, y)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 112, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\", line 389, in _fit_resample\n",
      "    nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 835, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples_fit, but n_neighbors = 6, n_samples_fit = 3, n_samples = 3\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2844 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 329, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 265, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1057, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 112, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\combine\\_smote_tomek.py\", line 157, in _fit_resample\n",
      "    X_res, y_res = self.smote_.fit_resample(X, y)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 112, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\", line 389, in _fit_resample\n",
      "    nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 835, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples_fit, but n_neighbors = 6, n_samples_fit = 1, n_samples = 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1422 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 329, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 265, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1057, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 112, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\combine\\_smote_tomek.py\", line 157, in _fit_resample\n",
      "    X_res, y_res = self.smote_.fit_resample(X, y)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 112, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\", line 389, in _fit_resample\n",
      "    nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 835, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples_fit, but n_neighbors = 6, n_samples_fit = 2, n_samples = 2\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.8  0.48 0.8  ... 0.6   nan  nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.9047619  0.59047619 0.9047619  ... 0.79047619        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 1\n",
      "n_candidates: 3318\n",
      "n_resources: 81\n",
      "Fitting 5 folds for each of 3318 candidates, totalling 16590 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "1105 fits failed out of a total of 16590.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1105 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 329, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 265, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1057, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 108, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\utils\\_validation.py\", line 571, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\utils\\_validation.py\", line 430, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.8    0.48   0.8    ... 0.8375    nan    nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.9047619  0.59047619 0.9047619  ... 0.85625           nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 2\n",
      "n_candidates: 1106\n",
      "n_resources: 243\n",
      "Fitting 5 folds for each of 1106 candidates, totalling 5530 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.8        0.48       0.8        ... 0.8625     0.85416667 0.85416667]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.9047619  0.59047619 0.9047619  ... 0.86701031 0.86804124 0.86701031]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 3\n",
      "n_candidates: 369\n",
      "n_resources: 729\n",
      "Fitting 5 folds for each of 369 candidates, totalling 1845 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.8        0.48       0.8        ... 0.84827586 0.84827586 0.84689655]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.9047619  0.59047619 0.9047619  ... 0.85317324 0.85283019 0.84219554]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 4\n",
      "n_candidates: 123\n",
      "n_resources: 2187\n",
      "Fitting 5 folds for each of 123 candidates, totalling 615 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.8        0.48       0.8        ... 0.83935927 0.8389016  0.8375286 ]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.9047619  0.59047619 0.9047619  ... 0.85626072 0.85626072 0.85408805]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 5\n",
      "n_candidates: 41\n",
      "n_resources: 6561\n",
      "Fitting 5 folds for each of 41 candidates, totalling 205 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.8        0.48       0.8        ... 0.84908537 0.84939024 0.84939024]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.9047619  0.59047619 0.9047619  ... 0.84912348 0.84851372 0.84862805]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 6\n",
      "n_candidates: 14\n",
      "n_resources: 19683\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.8        0.48       0.8        ... 0.84578252 0.84679878 0.84690041]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.9047619  0.59047619 0.9047619  ... 0.84336339 0.8449257  0.84504001]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 7\n",
      "n_candidates: 5\n",
      "n_resources: 59049\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.8        0.48       0.8        ... 0.84386747 0.84386747 0.84386747]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.9047619  0.59047619 0.9047619  ... 0.8482815  0.8482815  0.8482815 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 8\n",
      "n_candidates: 2\n",
      "n_resources: 177147\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.8        0.48       0.8        ... 0.84386747 0.48605709 0.21850369]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.9047619  0.59047619 0.9047619  ... 0.8482815  0.48671849 0.21841439]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__C': 10, 'classifier__gamma': 'scale', 'classifier__kernel': 'rbf', 'classifier__tol': 0.1, 'resampler': None}\n",
      "Best Cross-Validation Accuracy: 0.48605709126489416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning with Halving Grid Search\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, CondensedNearestNeighbour\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('resampler', None),  # Placeholder for resampling method\n",
    "    ('classifier', SVC(max_iter=10000, random_state=42))  # Model\n",
    "])\n",
    "\n",
    "param_grid_toy = {\n",
    "    'classifier__kernel': ['linear'],  # Kernel type\n",
    "    'classifier__C': [0.1],  # Regularization strength\n",
    "    # 'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization type  # not possible for SVM\n",
    "    # 'classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga', 'sag'],  # Optimization algorithm  # not possible for SVM\n",
    "    'classifier__tol': [1e-3],  # Tolerance for stopping criteria  # TODO if time allows\n",
    "    'resampler': [RandomUnderSampler(random_state=42)] # many other options: TomekLinks(), CondensedNearestNeighbour(), ...\n",
    "}\n",
    "param_grid_small = {\n",
    "    'classifier__kernel': ['linear'],  # Kernel type\n",
    "    'classifier__C': [0.01, 0.1, 1, 10],  # Regularization strength\n",
    "    'resampler': [RandomUnderSampler(random_state=42), RandomUnderSampler(random_state=42, sampling_strategy=0.5)] # many other options: TomekLinks(), CondensedNearestNeighbour(), ...\n",
    "}\n",
    "param_grid = [\n",
    "    {\n",
    "        'classifier__kernel': ['linear'],  # Kernel type\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "        'classifier__tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria \n",
    "        'resampler': [None,  # original, imbalanced dataset\n",
    "                      RandomUnderSampler(random_state=42), \n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.5), # sampling_strategy = number of minoority class instances / number of majority class instances\n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.25), \n",
    "                      RandomOverSampler(random_state=42), \n",
    "                      SMOTE(random_state=42), \n",
    "                      SMOTETomek(random_state=42)] \n",
    "    }, \n",
    "    {\n",
    "        'classifier__kernel': ['poly'],  # Kernel type\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "        'classifier__degree': [2, 3, 4, 5], # Degree of the polynomial kernel function (only for ‘poly’)\n",
    "        'classifier__gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001],  # Kernel coefficient (only for ‘rbf’, ‘poly’ and ‘sigmoid’)\n",
    "        'classifier__coef0': [-1.0, 0.0, 1.0],  # Independent term in kernel function (only for ‘poly’ and ‘sigmoid’)\n",
    "        'classifier__tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria  \n",
    "        'resampler': [None,  # original, imbalanced dataset\n",
    "                      RandomUnderSampler(random_state=42), \n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.5), # sampling_strategy = number of minoority class instances / number of majority class instances\n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.25), \n",
    "                      RandomOverSampler(random_state=42), \n",
    "                      SMOTE(random_state=42), \n",
    "                      SMOTETomek(random_state=42)] \n",
    "    }, \n",
    "    {\n",
    "        'classifier__kernel': ['rbf'],  # Kernel type\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "        'classifier__gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001],  # Kernel coefficient  # Kernel coefficient (only for ‘rbf’, ‘poly’ and ‘sigmoid’)\n",
    "        'classifier__tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria \n",
    "        'resampler': [None,  # original, imbalanced dataset\n",
    "                      RandomUnderSampler(random_state=42), \n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.5), # sampling_strategy = number of minoority class instances / number of majority class instances\n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.25), \n",
    "                      RandomOverSampler(random_state=42), \n",
    "                      SMOTE(random_state=42), \n",
    "                      SMOTETomek(random_state=42)] \n",
    "    }\n",
    "]\n",
    "\n",
    "# Set up HalvingGridSearchCV\n",
    "halving_grid_search = HalvingGridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,  # Parameter grid remains the same  \n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='accuracy',  # we want to optimize recall  # TODO decide with team\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the random search on training data\n",
    "halving_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", halving_grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", halving_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.20      0.32     21797\n",
      "         1.0       0.13      0.63      0.22      4078\n",
      "\n",
      "    accuracy                           0.27     25875\n",
      "   macro avg       0.44      0.42      0.27     25875\n",
      "weighted avg       0.65      0.27      0.30     25875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_halving_GS = classification_report(y_val, halving_grid_search.predict(X_val))\n",
    "print(\"Classification Report:\\n\", report_halving_GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model saved to '../models/support_vector_machine/svm_model_halving-grid_even-oversampling_accuracy_20241126_003933.pkl'\n"
     ]
    }
   ],
   "source": [
    "best_model_halving_grid = halving_grid_search.best_estimator_\n",
    "save_model(best_model_halving_grid, \"halving-grid_even-oversampling_accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the PCA datasets\n",
    "train_pca = pd.read_csv(\"../data/pca/dataset_train_pca.csv\")\n",
    "val_pca = pd.read_csv(\"../data/pca/dataset_val_pca.csv\")\n",
    "test_pca = pd.read_csv(\"../data/pca/dataset_test_pca.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the PCA datasets into features and target\n",
    "X_train_pca = train_pca.drop(columns=[\"Diabetes\"])\n",
    "y_train_pca = train_pca[\"Diabetes\"]\n",
    "\n",
    "X_val_pca = val_pca.drop(columns=[\"Diabetes\"])\n",
    "y_val_pca = val_pca[\"Diabetes\"]\n",
    "\n",
    "X_test_pca = test_pca.drop(columns=[\"Diabetes\"])\n",
    "y_test_pca = test_pca[\"Diabetes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "model_pca = SVC(C=100, kernel='linear')  # result of grid search TODO\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "model_pca.fit(X_train_pca, y_train_pca)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = model_pca.predict(X_val_pca)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_val_pca, y_val_pred)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "print(classification_report(y_val_pca, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA on all components has an infeasible time complexity for Support Vector Machines (at least for Salome's computer without a GPU). It needs more than 12 hours and was automatically cut off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only the best n components for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = 2  # TODO 5\n",
    "X_train_best_components = X_train_pca.iloc[:, :num_components]\n",
    "X_val_best_components = X_val_pca.iloc[:, :num_components]\n",
    "\n",
    "# Initialize the classifier\n",
    "model_pca_n = SVC(C=100, kernel='linear', verbose=True)  # result of grid search TODO\n",
    "\n",
    "# Train the classifier\n",
    "model_pca_n.fit(X_train_best_components, y_train_pca)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = model_pca_n.predict(X_val_best_components)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_val_pca, y_val_pred)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "print(classification_report(y_val_pca, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO also here: undersampling?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
