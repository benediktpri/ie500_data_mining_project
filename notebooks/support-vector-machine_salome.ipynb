{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))\n",
    "from data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader()\n",
    "X_train, y_train = data_loader.training_data\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (177576, 21)\n",
      "y_train shape: (177576,)\n",
      "X_val shape: (25875, 21)\n",
      "y_val shape: (25875,)\n",
      "X_test shape: (50229, 21)\n",
      "y_test shape: (50229,)\n",
      "Number of negative samples in training set: 149592\n",
      "Number of positive samples in training set: 27984\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "# (data == value_to_count) creates a boolean Series where True corresponds to occurrences of the specific value.\n",
    "# .sum() counts the True values (since True is equivalent to 1 in Python).\n",
    "print(f\"Number of negative samples in training set: {(y_train == 0.0).sum()}\")\n",
    "print(f\"Number of positive samples in training set: {(y_train == 1.0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Save model to pkl file for later reuse\n",
    "def save_model (model, model_name):\n",
    "    # Get the current timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Save the best model to a file with a timestamp\n",
    "    model_filename = f'../models/support_vector_machine/svm_model_{model_name}_{timestamp}.pkl'\n",
    "    joblib.dump(model, model_filename)\n",
    "\n",
    "    print(f\"Initial model saved to '{model_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we just try out the Support Vector Machine to get an initial feeling how it performs and to have something to improve upon in the following sections using Resampling, Hyperparameter Tuning, and PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the support vector machine model\n",
    "model_initial = SVC(C=1.0, kernel='rbf', verbose=True)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "model_initial.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = model_initial.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically the exact same result as for the majority classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it has such a long runtime (), we save it. Of course, we hope that we do not have to reuse it because we improve it in the following cells. But better safe than sorry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to pkl file for later reuse\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save the best model to a file with a timestamp\n",
    "model_filename = f'../models/support_vector_machine/svm_model_initial_{timestamp}.pkl'\n",
    "joblib.dump(model_initial, model_filename)\n",
    "\n",
    "print(f\"Initial model saved to '{model_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_undersampling_random shape: (55968, 21)\n",
      "y_train_undersampling_random shape: (55968,)\n",
      "X_val shape: (25875, 21)\n",
      "y_val shape: (25875,)\n",
      "X_test shape: (50229, 21)\n",
      "y_test shape: (50229,)\n",
      "Number of negative samples in undersampled training set: 27984\n",
      "Number of positive samples in undersampled training set: 27984\n"
     ]
    }
   ],
   "source": [
    "# test random undersampling\n",
    "X_train_undersampling_random, y_train_undersampling_random = data_loader.training_data_undersampling_random\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data\n",
    "\n",
    "print(f\"X_train_undersampling_random shape: {X_train_undersampling_random.shape}\")\n",
    "print(f\"y_train_undersampling_random shape: {y_train_undersampling_random.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# (data == value_to_count) creates a boolean Series where True corresponds to occurrences of the specific value.\n",
    "# .sum() counts the True values (since True is equivalent to 1 in Python).\n",
    "print(f\"Number of negative samples in undersampled training set: {(y_train_undersampling_random == 0.0).sum()}\")\n",
    "print(f\"Number of positive samples in undersampled training set: {(y_train_undersampling_random == 1.0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the undersampling, we have as many positive as negative examples in our training data. This more balanced training set might be useful to improve the recall on the positive class, i.e., diabetes (previously, the minority class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By random undersampling, we reduced the original training dataset containing 177567 examples to 55968 examples, which is approximately a third of the original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "# Initialize the support vector machine model\n",
    "model_undersampling = SVC(C=1.0, kernel='rbf', verbose=True)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "model_undersampling.fit(X_train_undersampling_random, y_train_undersampling_random)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred_undersampling = model_undersampling.predict(X_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6922125603864734\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.67      0.79     21797\n",
      "         1.0       0.32      0.81      0.45      4078\n",
      "\n",
      "    accuracy                           0.69     25875\n",
      "   macro avg       0.63      0.74      0.62     25875\n",
      "weighted avg       0.85      0.69      0.73     25875\n",
      "\n",
      "[16269 16310]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance\n",
    "accuracy_undersampling = accuracy_score(y_val, y_val_pred_undersampling)\n",
    "report_undersampling = classification_report(y_val, y_val_pred_undersampling)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy_undersampling}\")\n",
    "print(\"Classification Report:\\n\", report_undersampling)\n",
    "\n",
    "print(f\"Number of support vectors for each class: {model_undersampling.n_support_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduction of data points to a third drastically improves the runtime from over 40 minutes (TODO!) to 3 minutes (in our setting). This makes sense since the training time of a Support Vector Machine generally has a cubic runtime O(n^3), i.e., it grows cubically with the number n of training examples (source: https://stackoverflow.com/questions/18165213/how-much-time-does-it-take-to-train-a-svm-classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM on undersampled data yields a better recall for the positive class than SVM on the full training data. But as expected for higher recall, but the overall accuracy suffers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model saved to '../models/support_vector_machine/svm_model_undersampling_20241125_105832.pkl'\n"
     ]
    }
   ],
   "source": [
    "save_model(model_undersampling, \"undersampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper for theory why Undersampling is so good for SVMs: https://www.sciencedirect.com/science/article/pii/S1474667016429952"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: one or two other undersampling methods so that I can compare?\n",
    "# ideas:\n",
    "# - slighter/softer undersampling (i.e., majority class has just 2x more examples) -> slightly more examples\n",
    "# - undersampling the SMOTE tomek\n",
    "# - more ideas: https://imbalanced-learn.org/stable/references/under_sampling.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing random oversampling for SVMs would make training time even longer. For comparison, the original training dataset has 177567 samples and our undersampled dataset 63964 samples (i.e., approximately a third of the original). Our oversampled dataset (the random version and also the SMOTE version) has 299184 samples. Thus, is not performed for SVMs here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing SMOTE oversampling for SVMs would make training time even longer. For comparison, the original training dataset has 177567 samples and our undersampled dataset 63964 samples (i.e., approximately a third of the original). Our oversampled dataset (the random version and also the SMOTE version) has 299184 samples. Thus, is not performed for SVMs here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE Tomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_oversampling_smote shape: (298548, 21)\n",
      "y_train_oversampling_smote shape: (298548,)\n",
      "X_val shape: (25875, 21)\n",
      "y_val shape: (25875,)\n",
      "X_test shape: (50229, 21)\n",
      "y_test shape: (50229,)\n",
      "Number of negative samples in SMOTE Tomek training set: 149274\n",
      "Number of positive samples in SMOTE Tomek training set: 149274\n"
     ]
    }
   ],
   "source": [
    "X_train_oversampling_smote_tomek, y_train_oversampling_smote_tomek = data_loader.training_data_resampling_smote_tomek\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data\n",
    "\n",
    "print(f\"X_train_oversampling_smote shape: {X_train_oversampling_smote_tomek.shape}\")\n",
    "print(f\"y_train_oversampling_smote shape: {y_train_oversampling_smote_tomek.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"Number of negative samples in SMOTE Tomek training set: {(y_train_oversampling_smote_tomek == 0.0).sum()}\")\n",
    "print(f\"Number of positive samples in SMOTE Tomek training set: {(y_train_oversampling_smote_tomek == 1.0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the SMOTE Tomek sampling, we have as many positive as negative examples in our training data, as after the Random Undersampling. Here again, we hope that this more balanced training set might be useful to improve the recall on the positive class (i.e., diabetes) (previously, the minority class) compared with the original dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By SMOTE Tomek sampling, we increased the original training dataset containing 177567 examples to 298548 examples, which is roughly 1.5 times the size of the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "# Initialize the support vector machine model\n",
    "model_smote_tomek = SVC(C=1.0, kernel='rbf', verbose=True)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "model_smote_tomek.fit(X_train_oversampling_smote_tomek, y_train_oversampling_smote_tomek)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred_smote_tomek = model_smote_tomek.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy_smote_tomek = accuracy_score(y_val, y_val_pred_smote_tomek)\n",
    "report_smote_tomek = classification_report(y_val, y_val_pred_smote_tomek)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy_smote_tomek}\")\n",
    "print(\"Classification Report:\\n\", report_smote_tomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model_smote_tomek, \"smote_tomek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compare inparticular original, random undersampling, and SMOTE Tomek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tune the following hyperparameters: the type of kernel (e.g., linear, rbf, polynomial, sigmoid), the regularization parameter (C), and kernel-specific parameters like gamma for the RBF kernel and the degree for polynomial kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model of randomized search yields a worse accuracy than the best model of grid search. Also the f1-score is worse. Thus, it seems to make sense to go with the model identified by grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Halving Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to time complexity, hyperparameter tuning is only feasible with the halving grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 as objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 59192\n",
      "max_resources_: 177576\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 8\n",
      "n_resources: 59192\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 3\n",
      "n_resources: 177576\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 61\u001b[0m\n\u001b[0;32m     51\u001b[0m halving_grid_search \u001b[38;5;241m=\u001b[39m HalvingGridSearchCV(\n\u001b[0;32m     52\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mpipeline,\n\u001b[0;32m     53\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid_small,  \u001b[38;5;66;03m# Parameter grid remains the same  # TODO\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# To track progress\u001b[39;00m\n\u001b[0;32m     58\u001b[0m )\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Fit the random search on training data\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m halving_grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Get the best parameters and score\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, halving_grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py:251\u001b[0m, in \u001b[0;36mBaseSuccessiveHalving.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_input_parameters(\n\u001b[0;32m    246\u001b[0m     X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, split_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit\n\u001b[0;32m    247\u001b[0m )\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_samples_orig \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y\u001b[38;5;241m=\u001b[39my, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# Set best_score_: BaseSearchCV does not set it, as refit is a callable\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_score_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv_results_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_test_score\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_index_]\n",
      "File \u001b[1;32mc:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py:355\u001b[0m, in \u001b[0;36mBaseSuccessiveHalving._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m    348\u001b[0m     cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checked_cv_orig\n\u001b[0;32m    350\u001b[0m more_results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miter\u001b[39m\u001b[38;5;124m\"\u001b[39m: [itr] \u001b[38;5;241m*\u001b[39m n_candidates,\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_resources\u001b[39m\u001b[38;5;124m\"\u001b[39m: [n_resources] \u001b[38;5;241m*\u001b[39m n_candidates,\n\u001b[0;32m    353\u001b[0m }\n\u001b[1;32m--> 355\u001b[0m results \u001b[38;5;241m=\u001b[39m evaluate_candidates(\n\u001b[0;32m    356\u001b[0m     candidate_params, cv, more_results\u001b[38;5;241m=\u001b[39mmore_results\n\u001b[0;32m    357\u001b[0m )\n\u001b[0;32m    359\u001b[0m n_candidates_to_keep \u001b[38;5;241m=\u001b[39m ceil(n_candidates \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfactor)\n\u001b[0;32m    360\u001b[0m candidate_params \u001b[38;5;241m=\u001b[39m _top_k(results, n_candidates_to_keep, itr)\n",
      "File \u001b[1;32mc:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    917\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    918\u001b[0m         clone(base_estimator),\n\u001b[0;32m    919\u001b[0m         X,\n\u001b[0;32m    920\u001b[0m         y,\n\u001b[0;32m    921\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    922\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    923\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    924\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    925\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    927\u001b[0m     )\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    929\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    930\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    931\u001b[0m     )\n\u001b[0;32m    932\u001b[0m )\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning with Halving Grid Search\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, CondensedNearestNeighbour\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('resampler', None),  # Placeholder for resampling method\n",
    "    ('classifier', SVC(max_iter=10000, random_state=42, probability=True))  # Model\n",
    "])\n",
    "\n",
    "param_grid_toy = {\n",
    "    'classifier__kernel': ['linear'],  # Kernel type\n",
    "    'classifier__C': [0.1],  # Regularization strength\n",
    "    # 'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization type  # not possible for SVM\n",
    "    # 'classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga', 'sag'],  # Optimization algorithm  # not possible for SVM\n",
    "    'classifier__tol': [1e-3],  # Tolerance for stopping criteria  \n",
    "    'resampler': [RandomUnderSampler(random_state=42)] # many other options: TomekLinks(), CondensedNearestNeighbour(), ...\n",
    "}\n",
    "param_grid_small = {\n",
    "    'classifier__kernel': ['linear'],  # Kernel type\n",
    "    'classifier__C': [0.01, 0.1, 1, 10],  # Regularization strength\n",
    "    'resampler': [RandomUnderSampler(random_state=42), RandomUnderSampler(random_state=42, sampling_strategy=0.5)] # many other options: TomekLinks(), CondensedNearestNeighbour(), ...\n",
    "}\n",
    "param_grid = [\n",
    "    {\n",
    "        'classifier__kernel': ['linear'],  # Kernel type\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "        'classifier__tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria \n",
    "        'resampler': [RandomUnderSampler(random_state=42), RandomUnderSampler(random_state=42, sampling_strategy=0.5)] # many other options: TomekLinks(), CondensedNearestNeighbour(), ...\n",
    "    }, \n",
    "    {\n",
    "        'classifier__kernel': ['poly'],  # Kernel type\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "        'classifier__degree': [2, 3, 4, 5], # Degree of the polynomial kernel function (only for ‘poly’)\n",
    "        'classifier__gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001],  # Kernel coefficient (only for ‘rbf’, ‘poly’ and ‘sigmoid’)\n",
    "        'classifier__coef0': [-1.0, 0.0, 1.0],  # Independent term in kernel function (only for ‘poly’ and ‘sigmoid’)\n",
    "        'classifier__tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria \n",
    "        'resampler': [RandomUnderSampler(random_state=42), RandomUnderSampler(random_state=42, sampling_strategy=0.5)] # many other options: TomekLinks(), CondensedNearestNeighbour(), ...\n",
    "    }, \n",
    "    {\n",
    "        'classifier__kernel': ['rbf'],  # Kernel type\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "        'classifier__gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001],  # Kernel coefficient  # Kernel coefficient (only for ‘rbf’, ‘poly’ and ‘sigmoid’)\n",
    "        'classifier__tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria  # TODO if time allows\n",
    "        'resampler': [RandomUnderSampler(random_state=42), RandomUnderSampler(random_state=42, sampling_strategy=0.5)] # many other options: TomekLinks(), CondensedNearestNeighbour(), ...\n",
    "    }\n",
    "]\n",
    "\n",
    "# Set up HalvingGridSearchCV\n",
    "halving_grid_search = HalvingGridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid_small,  # Parameter grid remains the same  # TODO\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='f1',  # we want to optimize recall  # TODO decide with team\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the random search on training data\n",
    "halving_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", halving_grid_search.best_params_)\n",
    "print(\"Best Cross-Validation F1-Score:\", halving_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.00      0.00     21797\n",
      "         1.0       0.16      1.00      0.27      4078\n",
      "\n",
      "    accuracy                           0.16     25875\n",
      "   macro avg       0.44      0.50      0.14     25875\n",
      "weighted avg       0.64      0.16      0.05     25875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_halving_GS = classification_report(y_val, halving_grid_search.predict(X_val))\n",
    "print(\"Classification Report:\\n\", report_halving_GS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model saved to '../models/support_vector_machine/svm_model_halving-grid_small_20241125_184350.pkl'\n"
     ]
    }
   ],
   "source": [
    "best_model_halving_grid = halving_grid_search.best_estimator_\n",
    "save_model(best_model_halving_grid, \"halving-grid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy as objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with scoring='accuracy' and see whether the results are more balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 9\n",
      "n_required_iterations: 9\n",
      "n_possible_iterations: 9\n",
      "min_resources_: 27\n",
      "max_resources_: 177576\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 9954\n",
      "n_resources: 27\n",
      "Fitting 5 folds for each of 9954 candidates, totalling 49770 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "14220 fits failed out of a total of 49770.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2844 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 329, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 265, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1057, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 112, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\", line 389, in _fit_resample\n",
      "    nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 835, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples_fit, but n_neighbors = 6, n_samples_fit = 3, n_samples = 3\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2844 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 329, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 265, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1057, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 112, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\", line 389, in _fit_resample\n",
      "    nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 835, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples_fit, but n_neighbors = 6, n_samples_fit = 1, n_samples = 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1422 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 329, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 265, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1057, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 112, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\", line 389, in _fit_resample\n",
      "    nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 835, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples_fit, but n_neighbors = 6, n_samples_fit = 2, n_samples = 2\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2844 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 329, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 265, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1057, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 112, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\combine\\_smote_tomek.py\", line 157, in _fit_resample\n",
      "    X_res, y_res = self.smote_.fit_resample(X, y)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 112, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\", line 389, in _fit_resample\n",
      "    nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 835, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples_fit, but n_neighbors = 6, n_samples_fit = 3, n_samples = 3\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2844 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 329, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 265, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1057, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 112, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\combine\\_smote_tomek.py\", line 157, in _fit_resample\n",
      "    X_res, y_res = self.smote_.fit_resample(X, y)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 112, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\", line 389, in _fit_resample\n",
      "    nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 835, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples_fit, but n_neighbors = 6, n_samples_fit = 1, n_samples = 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1422 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 329, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 265, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1057, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 112, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\combine\\_smote_tomek.py\", line 157, in _fit_resample\n",
      "    X_res, y_res = self.smote_.fit_resample(X, y)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 112, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\", line 389, in _fit_resample\n",
      "    nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 835, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples_fit, but n_neighbors = 6, n_samples_fit = 2, n_samples = 2\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.8  0.48 0.8  ... 0.6   nan  nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.9047619  0.59047619 0.9047619  ... 0.79047619        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 1\n",
      "n_candidates: 3318\n",
      "n_resources: 81\n",
      "Fitting 5 folds for each of 3318 candidates, totalling 16590 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "1105 fits failed out of a total of 16590.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1105 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 329, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 265, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1057, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\base.py\", line 108, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\utils\\_validation.py\", line 571, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\imblearn\\utils\\_validation.py\", line 430, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.8    0.48   0.8    ... 0.8375    nan    nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.9047619  0.59047619 0.9047619  ... 0.85625           nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 2\n",
      "n_candidates: 1106\n",
      "n_resources: 243\n",
      "Fitting 5 folds for each of 1106 candidates, totalling 5530 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.8        0.48       0.8        ... 0.8625     0.85416667 0.85416667]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.9047619  0.59047619 0.9047619  ... 0.86701031 0.86804124 0.86701031]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 3\n",
      "n_candidates: 369\n",
      "n_resources: 729\n",
      "Fitting 5 folds for each of 369 candidates, totalling 1845 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.8        0.48       0.8        ... 0.84827586 0.84827586 0.84689655]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.9047619  0.59047619 0.9047619  ... 0.85317324 0.85283019 0.84219554]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 4\n",
      "n_candidates: 123\n",
      "n_resources: 2187\n",
      "Fitting 5 folds for each of 123 candidates, totalling 615 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.8        0.48       0.8        ... 0.83935927 0.8389016  0.8375286 ]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.9047619  0.59047619 0.9047619  ... 0.85626072 0.85626072 0.85408805]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 5\n",
      "n_candidates: 41\n",
      "n_resources: 6561\n",
      "Fitting 5 folds for each of 41 candidates, totalling 205 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.8        0.48       0.8        ... 0.84908537 0.84939024 0.84939024]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.9047619  0.59047619 0.9047619  ... 0.84912348 0.84851372 0.84862805]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 6\n",
      "n_candidates: 14\n",
      "n_resources: 19683\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.8        0.48       0.8        ... 0.84578252 0.84679878 0.84690041]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.9047619  0.59047619 0.9047619  ... 0.84336339 0.8449257  0.84504001]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 7\n",
      "n_candidates: 5\n",
      "n_resources: 59049\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.8        0.48       0.8        ... 0.84386747 0.84386747 0.84386747]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.9047619  0.59047619 0.9047619  ... 0.8482815  0.8482815  0.8482815 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 8\n",
      "n_candidates: 2\n",
      "n_resources: 177147\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.8        0.48       0.8        ... 0.84386747 0.48605709 0.21850369]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.9047619  0.59047619 0.9047619  ... 0.8482815  0.48671849 0.21841439]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__C': 10, 'classifier__gamma': 'scale', 'classifier__kernel': 'rbf', 'classifier__tol': 0.1, 'resampler': None}\n",
      "Best Cross-Validation Accuracy: 0.48605709126489416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning with Halving Grid Search\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, CondensedNearestNeighbour\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('resampler', None),  # Placeholder for resampling method\n",
    "    ('classifier', SVC(max_iter=10000, random_state=42, probability=True))  # Model\n",
    "])\n",
    "\n",
    "param_grid_toy = {\n",
    "    'classifier__kernel': ['linear'],  # Kernel type\n",
    "    'classifier__C': [0.1],  # Regularization strength\n",
    "    # 'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization type  # not possible for SVM\n",
    "    # 'classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga', 'sag'],  # Optimization algorithm  # not possible for SVM\n",
    "    'classifier__tol': [1e-3],  # Tolerance for stopping criteria \n",
    "    'resampler': [RandomUnderSampler(random_state=42)] # many other options: TomekLinks(), CondensedNearestNeighbour(), ...\n",
    "}\n",
    "param_grid_small = {\n",
    "    'classifier__kernel': ['linear'],  # Kernel type\n",
    "    'classifier__C': [0.01, 0.1, 1, 10],  # Regularization strength\n",
    "    'resampler': [RandomUnderSampler(random_state=42), RandomUnderSampler(random_state=42, sampling_strategy=0.5)] # many other options: TomekLinks(), CondensedNearestNeighbour(), ...\n",
    "}\n",
    "param_grid = [\n",
    "    {\n",
    "        'classifier__kernel': ['linear'],  # Kernel type\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "        'classifier__tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria \n",
    "        'resampler': [None,  # original, imbalanced dataset\n",
    "                      RandomUnderSampler(random_state=42), \n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.5), # sampling_strategy = number of minoority class instances / number of majority class instances\n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.25), \n",
    "                      RandomOverSampler(random_state=42), \n",
    "                      SMOTE(random_state=42), \n",
    "                      SMOTETomek(random_state=42)] \n",
    "    }, \n",
    "    {\n",
    "        'classifier__kernel': ['poly'],  # Kernel type\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "        'classifier__degree': [2, 3, 4, 5], # Degree of the polynomial kernel function (only for ‘poly’)\n",
    "        'classifier__gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001],  # Kernel coefficient (only for ‘rbf’, ‘poly’ and ‘sigmoid’)\n",
    "        'classifier__coef0': [-1.0, 0.0, 1.0],  # Independent term in kernel function (only for ‘poly’ and ‘sigmoid’)\n",
    "        'classifier__tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria  \n",
    "        'resampler': [None,  # original, imbalanced dataset\n",
    "                      RandomUnderSampler(random_state=42), \n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.5), # sampling_strategy = number of minoority class instances / number of majority class instances\n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.25), \n",
    "                      RandomOverSampler(random_state=42), \n",
    "                      SMOTE(random_state=42), \n",
    "                      SMOTETomek(random_state=42)] \n",
    "    }, \n",
    "    {\n",
    "        'classifier__kernel': ['rbf'],  # Kernel type\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "        'classifier__gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001],  # Kernel coefficient  # Kernel coefficient (only for ‘rbf’, ‘poly’ and ‘sigmoid’)\n",
    "        'classifier__tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria \n",
    "        'resampler': [None,  # original, imbalanced dataset\n",
    "                      RandomUnderSampler(random_state=42), \n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.5), # sampling_strategy = number of minoority class instances / number of majority class instances\n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.25), \n",
    "                      RandomOverSampler(random_state=42), \n",
    "                      SMOTE(random_state=42), \n",
    "                      SMOTETomek(random_state=42)] \n",
    "    }\n",
    "]\n",
    "\n",
    "# Set up HalvingGridSearchCV\n",
    "halving_grid_search = HalvingGridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,  # Parameter grid remains the same  \n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='accuracy',  # we want to optimize recall  # TODO decide with team\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the random search on training data\n",
    "halving_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", halving_grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", halving_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.20      0.32     21797\n",
      "         1.0       0.13      0.63      0.22      4078\n",
      "\n",
      "    accuracy                           0.27     25875\n",
      "   macro avg       0.44      0.42      0.27     25875\n",
      "weighted avg       0.65      0.27      0.30     25875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_halving_GS = classification_report(y_val, halving_grid_search.predict(X_val))\n",
    "print(\"Classification Report:\\n\", report_halving_GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model saved to '../models/support_vector_machine/svm_model_halving-grid_even-oversampling_accuracy_20241126_003933.pkl'\n"
     ]
    }
   ],
   "source": [
    "best_model_halving_grid = halving_grid_search.best_estimator_\n",
    "save_model(best_model_halving_grid, \"halving-grid_even-oversampling_accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain best model with probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate probability outputs, i.e., SVC(probability=True). We do this to gain more insight into the model during evaluation (e.g., better precision recall curves). Since this increases the training time significantly, and does not change the performance of the model, this is not done during Hyperparameter Tuning, but now, just for the best model resulting from the Hyperparameter Tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 1\n",
      "n_required_iterations: 1\n",
      "n_possible_iterations: 1\n",
      "min_resources_: 177576\n",
      "max_resources_: 177576\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 1\n",
      "n_resources: 177576\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best Parameters: {'classifier__C': 0.01, 'classifier__coef0': -1.0, 'classifier__degree': 5, 'classifier__gamma': 0.0001, 'classifier__kernel': 'poly', 'classifier__tol': 0.1, 'resampler': RandomUnderSampler(random_state=42)}\n",
      "Best Cross-Validation Accuracy: 0.23306622137471456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning with Halving Grid Search\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, CondensedNearestNeighbour\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "pipeline_probability = Pipeline([\n",
    "    ('resampler', None),  # Placeholder for resampling method\n",
    "    ('classifier', SVC(max_iter=10000, random_state=42, probability=True))  # Model, now with probabilities\n",
    "])\n",
    "\n",
    "param_grid_probability = {\n",
    "    'classifier__C': [0.01], \n",
    "    'classifier__coef0': [-1.0], \n",
    "    'classifier__degree': [5], \n",
    "    'classifier__gamma': [0.0001], \n",
    "    'classifier__kernel': ['poly'], \n",
    "    'classifier__tol': [0.1], \n",
    "    'resampler': [RandomUnderSampler(random_state=42)] \n",
    "    }\n",
    "\n",
    "# Set up HalvingGridSearchCV\n",
    "halving_grid_search_probability = HalvingGridSearchCV(\n",
    "    estimator=pipeline_probability,\n",
    "    param_grid=param_grid_probability,  # Parameter grid remains the same \n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='accuracy',  # we want to optimize recall  # TODO decide with team\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the random search on training data\n",
    "halving_grid_search_probability.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", halving_grid_search_probability.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", halving_grid_search_probability.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.06      0.11     21797\n",
      "         1.0       0.16      0.99      0.28      4078\n",
      "\n",
      "    accuracy                           0.21     25875\n",
      "   macro avg       0.56      0.52      0.20     25875\n",
      "weighted avg       0.84      0.21      0.14     25875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_halving_GS_probability = classification_report(y_val, halving_grid_search_probability.predict(X_val))\n",
    "print(\"Classification Report:\\n\", report_halving_GS_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model saved to '../models/support_vector_machine/svm_model__with-probability_undersampling__20241127_123528.pkl'\n"
     ]
    }
   ],
   "source": [
    "model_with_probability = halving_grid_search_probability.best_estimator_\n",
    "save_model(model_with_probability, \"_with-probability_undersampling_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the full training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 1\n",
      "n_required_iterations: 1\n",
      "n_possible_iterations: 1\n",
      "min_resources_: 177576\n",
      "max_resources_: 177576\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 1\n",
      "n_resources: 177576\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best Parameters: {'classifier__C': 0.01, 'classifier__coef0': -1.0, 'classifier__degree': 5, 'classifier__gamma': 0.0001, 'classifier__kernel': 'poly', 'classifier__tol': 0.1, 'resampler': None}\n",
      "Best Cross-Validation Accuracy: 0.5165336620011021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salome Heckenthaler\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning with Halving Grid Search\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, CondensedNearestNeighbour\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "pipeline_probability = Pipeline([\n",
    "    ('resampler', None),  # Placeholder for resampling method\n",
    "    ('classifier', SVC(max_iter=10000, random_state=42, probability=True))  # Model, now with probabilities\n",
    "])\n",
    "\n",
    "param_grid_probability = {\n",
    "    'classifier__C': [0.01], \n",
    "    'classifier__coef0': [-1.0], \n",
    "    'classifier__degree': [5], \n",
    "    'classifier__gamma': [0.0001], \n",
    "    'classifier__kernel': ['poly'], \n",
    "    'classifier__tol': [0.1], \n",
    "    'resampler': [None] \n",
    "    }\n",
    "\n",
    "# Set up HalvingGridSearchCV\n",
    "halving_grid_search_probability = HalvingGridSearchCV(\n",
    "    estimator=pipeline_probability,\n",
    "    param_grid=param_grid_probability,  # Parameter grid remains the same  \n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='accuracy',  # we want to optimize recall  # TODO decide with team\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the random search on training data\n",
    "halving_grid_search_probability.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", halving_grid_search_probability.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", halving_grid_search_probability.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.47      0.63     21797\n",
      "         1.0       0.24      0.88      0.37      4078\n",
      "\n",
      "    accuracy                           0.54     25875\n",
      "   macro avg       0.60      0.67      0.50     25875\n",
      "weighted avg       0.84      0.54      0.59     25875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_halving_GS_probability = classification_report(y_val, halving_grid_search_probability.predict(X_val))\n",
    "print(\"Classification Report:\\n\", report_halving_GS_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model saved to '../models/support_vector_machine/svm_model__with-probability_full-data__20241127_155642.pkl'\n"
     ]
    }
   ],
   "source": [
    "model_with_probability = halving_grid_search_probability.best_estimator_\n",
    "save_model(model_with_probability, \"_with-probability_full-data_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the PCA datasets\n",
    "train_pca = pd.read_csv(\"../data/pca/dataset_train_pca.csv\")\n",
    "val_pca = pd.read_csv(\"../data/pca/dataset_val_pca.csv\")\n",
    "test_pca = pd.read_csv(\"../data/pca/dataset_test_pca.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the PCA datasets into features and target\n",
    "X_train_pca = train_pca.drop(columns=[\"Diabetes\"])\n",
    "y_train_pca = train_pca[\"Diabetes\"]\n",
    "\n",
    "X_val_pca = val_pca.drop(columns=[\"Diabetes\"])\n",
    "y_val_pca = val_pca[\"Diabetes\"]\n",
    "\n",
    "X_test_pca = test_pca.drop(columns=[\"Diabetes\"])\n",
    "y_test_pca = test_pca[\"Diabetes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "model_pca = SVC(C=100, kernel='linear')  # result of grid search TODO\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "model_pca.fit(X_train_pca, y_train_pca)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = model_pca.predict(X_val_pca)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_val_pca, y_val_pred)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "print(classification_report(y_val_pca, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA on all components has an infeasible time complexity for Support Vector Machines (at least for Salome's computer without a GPU). It needs more than 12 hours and was automatically cut off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only the best n components for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = 2  # TODO 5\n",
    "X_train_best_components = X_train_pca.iloc[:, :num_components]\n",
    "X_val_best_components = X_val_pca.iloc[:, :num_components]\n",
    "\n",
    "# Initialize the classifier\n",
    "model_pca_n = SVC(C=100, kernel='linear', verbose=True)  # result of grid search TODO\n",
    "\n",
    "# Train the classifier\n",
    "model_pca_n.fit(X_train_best_components, y_train_pca)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = model_pca_n.predict(X_val_best_components)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_val_pca, y_val_pred)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "print(classification_report(y_val_pca, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO also here: undersampling?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
