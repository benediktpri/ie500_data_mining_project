{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suppress the warnings, so that the outputs of our models are reduced to the relevant parts and the notebook does not get unneccessarily long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))\n",
    "from data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader()\n",
    "X_train, y_train = data_loader.training_data\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (177576, 21)\n",
      "y_train shape: (177576,)\n",
      "X_val shape: (25875, 21)\n",
      "y_val shape: (25875,)\n",
      "X_test shape: (50229, 21)\n",
      "y_test shape: (50229,)\n",
      "Number of negative samples in training set: 149592\n",
      "Number of positive samples in training set: 27984\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "# (data == value_to_count) creates a boolean Series where True corresponds to occurrences of the specific value.\n",
    "# .sum() counts the True values (since True is equivalent to 1 in Python).\n",
    "print(f\"Number of negative samples in training set: {(y_train == 0.0).sum()}\")\n",
    "print(f\"Number of positive samples in training set: {(y_train == 1.0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Save model to pkl file for later reuse\n",
    "def save_model (model, model_name):\n",
    "    # Get the current timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Save the best model to a file with a timestamp\n",
    "    model_filename = f'../models/support_vector_machine/svm_model_{model_name}_{timestamp}.pkl'\n",
    "    joblib.dump(model, model_filename)\n",
    "\n",
    "    print(f\"Initial model saved to '{model_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we just try out the Support Vector Machine to get an initial feeling how it performs and to have something to improve upon in the following sections using Resampling, Hyperparameter Tuning, and PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we just use the default hyperparameters of the SVC (Support Vector Classifier) by scikit learn. Those are: \n",
    "\n",
    "- C=1.0\n",
    "- kernel='rbf'\n",
    "- degree=3\n",
    "- gamma='scale'\n",
    "- coef0=0.0\n",
    "- shrinking=True\n",
    "- probability=False\n",
    "- tol=1e-3\n",
    "- chacke_size=200\n",
    "- class_weight=None\n",
    "- verbose=False\n",
    "- max_iter=-1\n",
    "- decision_function_shape='ovr'\n",
    "- break_ties=False\n",
    "- random_state=None\n",
    "\n",
    "Source: https://scikit-learn.org/dev/modules/generated/sklearn.svm.SVC.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8423961352657005\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      1.00      0.91     21797\n",
      "         1.0       0.00      0.00      0.00      4078\n",
      "\n",
      "    accuracy                           0.84     25875\n",
      "   macro avg       0.42      0.50      0.46     25875\n",
      "weighted avg       0.71      0.84      0.77     25875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the support vector machine model\n",
    "model_initial = SVC()  # use default choices for a Support Vector Classifier\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "model_initial.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = model_initial.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically the exact same result as for the majority classifier. So, we used almost an whole hour (58 minutes on Salome's computer) and lots of energy for training to get the exact same result as with the majority classifier, that had almost zero training time. According to Occam's razor, we would never pick this SVM model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future evaluation, we save this model. Of course, we hope that we do not have to reuse it because we want to improve on it in the following cells. But better safe than sorry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model saved to '../models/support_vector_machine/svm_model_default-hyperparameters_20241129_164730.pkl'\n"
     ]
    }
   ],
   "source": [
    "save_model(model_initial, \"default-hyperparameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_undersampling_random shape: (55968, 21)\n",
      "y_train_undersampling_random shape: (55968,)\n",
      "X_val shape: (25875, 21)\n",
      "y_val shape: (25875,)\n",
      "X_test shape: (50229, 21)\n",
      "y_test shape: (50229,)\n",
      "Number of negative samples in undersampled training set: 27984\n",
      "Number of positive samples in undersampled training set: 27984\n"
     ]
    }
   ],
   "source": [
    "# test random undersampling\n",
    "X_train_undersampling_random, y_train_undersampling_random = data_loader.training_data_undersampling_random\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data\n",
    "\n",
    "print(f\"X_train_undersampling_random shape: {X_train_undersampling_random.shape}\")\n",
    "print(f\"y_train_undersampling_random shape: {y_train_undersampling_random.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# (data == value_to_count) creates a boolean Series where True corresponds to occurrences of the specific value.\n",
    "# .sum() counts the True values (since True is equivalent to 1 in Python).\n",
    "print(f\"Number of negative samples in undersampled training set: {(y_train_undersampling_random == 0.0).sum()}\")\n",
    "print(f\"Number of positive samples in undersampled training set: {(y_train_undersampling_random == 1.0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the undersampling, we have as many positive as negative examples in our training data. This more balanced training set might be useful to improve the recall on the positive class, i.e., diabetes (previously, the minority class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By random undersampling, we reduced the original training dataset containing 177567 examples to 55968 examples, which is approximately a third of the original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "# Initialize the support vector machine model\n",
    "model_undersampling = SVC(C=1.0, kernel='rbf', verbose=True)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "model_undersampling.fit(X_train_undersampling_random, y_train_undersampling_random)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred_undersampling = model_undersampling.predict(X_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6922125603864734\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.67      0.79     21797\n",
      "         1.0       0.32      0.81      0.45      4078\n",
      "\n",
      "    accuracy                           0.69     25875\n",
      "   macro avg       0.63      0.74      0.62     25875\n",
      "weighted avg       0.85      0.69      0.73     25875\n",
      "\n",
      "[16269 16310]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance\n",
    "accuracy_undersampling = accuracy_score(y_val, y_val_pred_undersampling)\n",
    "report_undersampling = classification_report(y_val, y_val_pred_undersampling)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy_undersampling}\")\n",
    "print(\"Classification Report:\\n\", report_undersampling)\n",
    "\n",
    "print(f\"Number of support vectors for each class: {model_undersampling.n_support_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduction of data points to a third drastically improves the runtime from over 40 minutes (TODO!) to 3 minutes (in our setting). This makes sense since the training time of a Support Vector Machine generally has a cubic runtime O(n^3), i.e., it grows cubically with the number n of training examples (source: https://stackoverflow.com/questions/18165213/how-much-time-does-it-take-to-train-a-svm-classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM on undersampled data yields a better recall for the positive class than SVM on the full training data. But as expected for higher recall, but the overall accuracy suffers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model saved to '../models/support_vector_machine/svm_model_undersampling_20241125_105832.pkl'\n"
     ]
    }
   ],
   "source": [
    "save_model(model_undersampling, \"undersampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper for theory why Undersampling is so good for SVMs: https://www.sciencedirect.com/science/article/pii/S1474667016429952"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: one or two other undersampling methods so that I can compare?\n",
    "# ideas:\n",
    "# - slighter/softer undersampling (i.e., majority class has just 2x more examples) -> slightly more examples\n",
    "# - undersampling the SMOTE tomek\n",
    "# - more ideas: https://imbalanced-learn.org/stable/references/under_sampling.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing random oversampling for SVMs would make training time even longer. For comparison, the original training dataset has 177567 samples and our undersampled dataset 63964 samples (i.e., approximately a third of the original). Our oversampled dataset (the random version and also the SMOTE version) has 299184 samples. Thus, is not performed for SVMs here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing SMOTE oversampling for SVMs would make training time even longer. For comparison, the original training dataset has 177567 samples and our undersampled dataset 63964 samples (i.e., approximately a third of the original). Our oversampled dataset (the random version and also the SMOTE version) has 299184 samples. Thus, is not performed for SVMs here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE Tomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_oversampling_smote shape: (298548, 21)\n",
      "y_train_oversampling_smote shape: (298548,)\n",
      "X_val shape: (25875, 21)\n",
      "y_val shape: (25875,)\n",
      "X_test shape: (50229, 21)\n",
      "y_test shape: (50229,)\n",
      "Number of negative samples in SMOTE Tomek training set: 149274\n",
      "Number of positive samples in SMOTE Tomek training set: 149274\n"
     ]
    }
   ],
   "source": [
    "X_train_oversampling_smote_tomek, y_train_oversampling_smote_tomek = data_loader.training_data_resampling_smote_tomek\n",
    "X_val, y_val = data_loader.validation_data\n",
    "X_test, y_test = data_loader.test_data\n",
    "\n",
    "print(f\"X_train_oversampling_smote shape: {X_train_oversampling_smote_tomek.shape}\")\n",
    "print(f\"y_train_oversampling_smote shape: {y_train_oversampling_smote_tomek.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"Number of negative samples in SMOTE Tomek training set: {(y_train_oversampling_smote_tomek == 0.0).sum()}\")\n",
    "print(f\"Number of positive samples in SMOTE Tomek training set: {(y_train_oversampling_smote_tomek == 1.0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the SMOTE Tomek sampling, we have as many positive as negative examples in our training data, as after the Random Undersampling. Here again, we hope that this more balanced training set might be useful to improve the recall on the positive class (i.e., diabetes) (previously, the minority class) compared with the original dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By SMOTE Tomek sampling, we increased the original training dataset containing 177567 examples to 298548 examples, which is roughly 1.5 times the size of the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "# Initialize the support vector machine model\n",
    "model_smote_tomek = SVC(C=1.0, kernel='rbf', verbose=True)\n",
    "\n",
    "# Train the model on the preprocessed training data\n",
    "model_smote_tomek.fit(X_train_oversampling_smote_tomek, y_train_oversampling_smote_tomek)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred_smote_tomek = model_smote_tomek.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy_smote_tomek = accuracy_score(y_val, y_val_pred_smote_tomek)\n",
    "report_smote_tomek = classification_report(y_val, y_val_pred_smote_tomek)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy_smote_tomek}\")\n",
    "print(\"Classification Report:\\n\", report_smote_tomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model_smote_tomek, \"smote_tomek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compare inparticular original, random undersampling, and SMOTE Tomek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tune the following hyperparameters: the type of kernel (e.g., linear, rbf, polynomial, sigmoid), the regularization parameter (C), and kernel-specific parameters like gamma for the RBF kernel and the degree for polynomial kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model of randomized search yields a worse accuracy than the best model of grid search. Also the f1-score is worse. Thus, it seems to make sense to go with the model identified by grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Halving Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to time complexity, hyperparameter tuning is only feasible with the halving grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with Halving Grid Search\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, CondensedNearestNeighbour\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('resampler', None),  # Placeholder for resampling method\n",
    "    ('classifier', SVC(max_iter=10000, random_state=42, probability=True))  # Model\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'classifier__kernel': ['linear'],  # Kernel type\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "        'classifier__tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria \n",
    "        'resampler': [None,  # original, imbalanced dataset\n",
    "                      RandomUnderSampler(random_state=42), \n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.5), # sampling_strategy = number of minoority class instances / number of majority class instances\n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.25), \n",
    "                      RandomOverSampler(random_state=42), \n",
    "                      SMOTE(random_state=42), \n",
    "                      SMOTETomek(random_state=42)] \n",
    "    }, \n",
    "    {\n",
    "        'classifier__kernel': ['poly'],  # Kernel type\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "        'classifier__degree': [2, 3, 4, 5], # Degree of the polynomial kernel function (only for ‘poly’)\n",
    "        'classifier__gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001],  # Kernel coefficient (only for ‘rbf’, ‘poly’ and ‘sigmoid’)\n",
    "        'classifier__coef0': [-1.0, 0.0, 1.0],  # Independent term in kernel function (only for ‘poly’ and ‘sigmoid’)\n",
    "        'classifier__tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria  \n",
    "        'resampler': [None,  # original, imbalanced dataset\n",
    "                      RandomUnderSampler(random_state=42), \n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.5), # sampling_strategy = number of minoority class instances / number of majority class instances\n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.25), \n",
    "                      RandomOverSampler(random_state=42), \n",
    "                      SMOTE(random_state=42), \n",
    "                      SMOTETomek(random_state=42)] \n",
    "    }, \n",
    "    {\n",
    "        'classifier__kernel': ['rbf'],  # Kernel type\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "        'classifier__gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001],  # Kernel coefficient  # Kernel coefficient (only for ‘rbf’, ‘poly’ and ‘sigmoid’)\n",
    "        'classifier__tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria \n",
    "        'resampler': [None,  # original, imbalanced dataset\n",
    "                      RandomUnderSampler(random_state=42), \n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.5), # sampling_strategy = number of minoority class instances / number of majority class instances\n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.25), \n",
    "                      RandomOverSampler(random_state=42), \n",
    "                      SMOTE(random_state=42), \n",
    "                      SMOTETomek(random_state=42)] \n",
    "    },\n",
    "    {\n",
    "        'classifier__kernel': ['sigmoid'],  # Kernel type\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "        'classifier__gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001],  # Kernel coefficient (only for ‘rbf’, ‘poly’ and ‘sigmoid’)\n",
    "        'classifier__coef0': [-1.0, 0.0, 1.0],  # Independent term in kernel function (only for ‘poly’ and ‘sigmoid’)\n",
    "        'classifier__tol': [1e-3, 1e-2, 1e-1],  # Tolerance for stopping criteria \n",
    "        'resampler': [None,  # original, imbalanced dataset\n",
    "                      RandomUnderSampler(random_state=42), \n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.5), # sampling_strategy = number of minoority class instances / number of majority class instances\n",
    "                      RandomUnderSampler(random_state=42, sampling_strategy=0.25), \n",
    "                      RandomOverSampler(random_state=42), \n",
    "                      SMOTE(random_state=42), \n",
    "                      SMOTETomek(random_state=42)] \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall as objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 9\n",
      "n_required_iterations: 9\n",
      "n_possible_iterations: 9\n",
      "min_resources_: 27\n",
      "max_resources_: 177576\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 12222\n",
      "n_resources: 27\n",
      "Fitting 5 folds for each of 12222 candidates, totalling 61110 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 4074\n",
      "n_resources: 81\n",
      "Fitting 5 folds for each of 4074 candidates, totalling 20370 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 1358\n",
      "n_resources: 243\n",
      "Fitting 5 folds for each of 1358 candidates, totalling 6790 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 453\n",
      "n_resources: 729\n",
      "Fitting 5 folds for each of 453 candidates, totalling 2265 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 151\n",
      "n_resources: 2187\n",
      "Fitting 5 folds for each of 151 candidates, totalling 755 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 51\n",
      "n_resources: 6561\n",
      "Fitting 5 folds for each of 51 candidates, totalling 255 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 17\n",
      "n_resources: 19683\n",
      "Fitting 5 folds for each of 17 candidates, totalling 85 fits\n",
      "----------\n",
      "iter: 7\n",
      "n_candidates: 6\n",
      "n_resources: 59049\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 8\n",
      "n_candidates: 2\n",
      "n_resources: 177147\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Best Parameters (Recall): {'classifier__C': 100, 'classifier__coef0': 0.0, 'classifier__gamma': 1, 'classifier__kernel': 'sigmoid', 'classifier__tol': 0.01, 'resampler': RandomOverSampler(random_state=42)}\n",
      "Best Cross-Validation (Recall): 1.0\n"
     ]
    }
   ],
   "source": [
    "# Set up HalvingGridSearchCV\n",
    "halving_grid_search_r = HalvingGridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,  # Parameter grid remains the same  \n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='recall',  # we want to optimize recall  # TODO decide with team\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the random search on training data\n",
    "halving_grid_search_r.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters (Recall):\", halving_grid_search_r.best_params_)\n",
    "print(\"Best Cross-Validation (Recall):\", halving_grid_search_r.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     21797\n",
      "         1.0       0.16      1.00      0.27      4078\n",
      "\n",
      "    accuracy                           0.16     25875\n",
      "   macro avg       0.08      0.50      0.14     25875\n",
      "weighted avg       0.02      0.16      0.04     25875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_halving_GS = classification_report(y_val, halving_grid_search_r.predict(X_val))\n",
    "print(\"Classification Report:\\n\", report_halving_GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model saved to '../models/support_vector_machine/svm_model_best-recall_20241129_074200.pkl'\n"
     ]
    }
   ],
   "source": [
    "best_model_r = halving_grid_search_r.best_estimator_\n",
    "save_model(best_model_r, \"best-recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 as objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with scoring='f1' and see whether the results are more balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 9\n",
      "n_required_iterations: 9\n",
      "n_possible_iterations: 9\n",
      "min_resources_: 27\n",
      "max_resources_: 177576\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 12222\n",
      "n_resources: 27\n",
      "Fitting 5 folds for each of 12222 candidates, totalling 61110 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 4074\n",
      "n_resources: 81\n",
      "Fitting 5 folds for each of 4074 candidates, totalling 20370 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 1358\n",
      "n_resources: 243\n",
      "Fitting 5 folds for each of 1358 candidates, totalling 6790 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 453\n",
      "n_resources: 729\n",
      "Fitting 5 folds for each of 453 candidates, totalling 2265 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 151\n",
      "n_resources: 2187\n",
      "Fitting 5 folds for each of 151 candidates, totalling 755 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 51\n",
      "n_resources: 6561\n",
      "Fitting 5 folds for each of 51 candidates, totalling 255 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 17\n",
      "n_resources: 19683\n",
      "Fitting 5 folds for each of 17 candidates, totalling 85 fits\n",
      "----------\n",
      "iter: 7\n",
      "n_candidates: 6\n",
      "n_resources: 59049\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 8\n",
      "n_candidates: 2\n",
      "n_resources: 177147\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Best Parameters (F1-Score): {'classifier__C': 0.1, 'classifier__coef0': 0.0, 'classifier__degree': 2, 'classifier__gamma': 0.1, 'classifier__kernel': 'poly', 'classifier__tol': 0.01, 'resampler': RandomUnderSampler(random_state=42, sampling_strategy=0.5)}\n",
      "Best Cross-Validation (F1-Score): 0.2829507177031396\n"
     ]
    }
   ],
   "source": [
    "# Set up HalvingGridSearchCV\n",
    "halving_grid_search_f1 = HalvingGridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,  # Parameter grid remains the same  \n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='f1',  # we want to optimize recall  # TODO decide with team\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the random search on training data\n",
    "halving_grid_search_f1.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters (F1-Score):\", halving_grid_search_f1.best_params_)\n",
    "print(\"Best Cross-Validation (F1-Score):\", halving_grid_search_f1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.45      0.60     21797\n",
      "         1.0       0.19      0.68      0.29      4078\n",
      "\n",
      "    accuracy                           0.49     25875\n",
      "   macro avg       0.53      0.56      0.44     25875\n",
      "weighted avg       0.77      0.49      0.55     25875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_halving_GS = classification_report(y_val, halving_grid_search_f1.predict(X_val))\n",
    "print(\"Classification Report:\\n\", report_halving_GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model saved to '../models/support_vector_machine/svm_model_best-f1_20241129_100911.pkl'\n"
     ]
    }
   ],
   "source": [
    "best_model_f1 = halving_grid_search_f1.best_estimator_\n",
    "save_model(best_model_f1, \"best-f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy as objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with scoring='accuracy' and see whether the results are more balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 9\n",
      "n_required_iterations: 9\n",
      "n_possible_iterations: 9\n",
      "min_resources_: 27\n",
      "max_resources_: 177576\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 12222\n",
      "n_resources: 27\n",
      "Fitting 5 folds for each of 12222 candidates, totalling 61110 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 4074\n",
      "n_resources: 81\n",
      "Fitting 5 folds for each of 4074 candidates, totalling 20370 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 1358\n",
      "n_resources: 243\n",
      "Fitting 5 folds for each of 1358 candidates, totalling 6790 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 453\n",
      "n_resources: 729\n",
      "Fitting 5 folds for each of 453 candidates, totalling 2265 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 151\n",
      "n_resources: 2187\n",
      "Fitting 5 folds for each of 151 candidates, totalling 755 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 51\n",
      "n_resources: 6561\n",
      "Fitting 5 folds for each of 51 candidates, totalling 255 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 17\n",
      "n_resources: 19683\n",
      "Fitting 5 folds for each of 17 candidates, totalling 85 fits\n",
      "----------\n",
      "iter: 7\n",
      "n_candidates: 6\n",
      "n_resources: 59049\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 8\n",
      "n_candidates: 2\n",
      "n_resources: 177147\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Best Parameters (Accuracy): {'classifier__C': 0.1, 'classifier__coef0': 1.0, 'classifier__degree': 4, 'classifier__gamma': 0.001, 'classifier__kernel': 'poly', 'classifier__tol': 0.1, 'resampler': None}\n",
      "Best Cross-Validation (Accuracy): 0.27219364195389784\n"
     ]
    }
   ],
   "source": [
    "# Set up HalvingGridSearchCV\n",
    "halving_grid_search_acc = HalvingGridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,  # Parameter grid remains the same  \n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='accuracy',  # we want to optimize recall  # TODO decide with team\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    verbose=1  # To track progress\n",
    ")\n",
    "\n",
    "# Fit the random search on training data\n",
    "halving_grid_search_acc.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters (Accuracy):\", halving_grid_search_acc.best_params_)\n",
    "print(\"Best Cross-Validation (Accuracy):\", halving_grid_search_acc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.12      0.21     21797\n",
      "         1.0       0.17      0.97      0.29      4078\n",
      "\n",
      "    accuracy                           0.25     25875\n",
      "   macro avg       0.56      0.54      0.25     25875\n",
      "weighted avg       0.83      0.25      0.22     25875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_halving_GS = classification_report(y_val, halving_grid_search_acc.predict(X_val))\n",
    "print(\"Classification Report:\\n\", report_halving_GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model saved to '../models/support_vector_machine/svm_model_best-accuracy_20241129_134911.pkl'\n"
     ]
    }
   ],
   "source": [
    "best_model_acc = halving_grid_search_acc.best_estimator_\n",
    "save_model(best_model_acc, \"best-accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain best model with probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate probability outputs, i.e., SVC(probability=True). We do this to gain more insight into the model during evaluation (e.g., better precision recall curves). Since this increases the training time significantly, and does not change the performance of the model, this is not done during Hyperparameter Tuning, but now, just for the best model resulting from the Hyperparameter Tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameter tuning with Halving Grid Search\n",
    "# from sklearn.experimental import enable_halving_search_cv\n",
    "# from imblearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import HalvingGridSearchCV\n",
    "# from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler, TomekLinks, CondensedNearestNeighbour\n",
    "# from imblearn.combine import SMOTETomek\n",
    "\n",
    "# pipeline_probability = Pipeline([\n",
    "#     ('resampler', None),  # Placeholder for resampling method\n",
    "#     ('classifier', SVC(max_iter=10000, random_state=42, probability=True))  # Model, now with probabilities\n",
    "# ])\n",
    "\n",
    "# param_grid_probability = {\n",
    "#     'classifier__C': [0.01], \n",
    "#     'classifier__coef0': [-1.0], \n",
    "#     'classifier__degree': [5], \n",
    "#     'classifier__gamma': [0.0001], \n",
    "#     'classifier__kernel': ['poly'], \n",
    "#     'classifier__tol': [0.1], \n",
    "#     'resampler': [RandomUnderSampler(random_state=42)] \n",
    "#     }\n",
    "\n",
    "# # Set up HalvingGridSearchCV\n",
    "# halving_grid_search_probability = HalvingGridSearchCV(\n",
    "#     estimator=pipeline_probability,\n",
    "#     param_grid=param_grid_probability,  # Parameter grid remains the same \n",
    "#     cv=5,  # 5-fold cross-validation\n",
    "#     scoring='accuracy',  # we want to optimize recall  # TODO decide with team\n",
    "#     n_jobs=-1,  # Use all processors\n",
    "#     verbose=1  # To track progress\n",
    "# )\n",
    "\n",
    "# # Fit the random search on training data\n",
    "# halving_grid_search_probability.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best parameters and score\n",
    "# print(\"Best Parameters:\", halving_grid_search_probability.best_params_)\n",
    "# print(\"Best Cross-Validation Accuracy:\", halving_grid_search_probability.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report_halving_GS_probability = classification_report(y_val, halving_grid_search_probability.predict(X_val))\n",
    "# print(\"Classification Report:\\n\", report_halving_GS_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_with_probability = halving_grid_search_probability.best_estimator_\n",
    "# save_model(model_with_probability, \"_with-probability_undersampling_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the full training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameter tuning with Halving Grid Search\n",
    "# from sklearn.experimental import enable_halving_search_cv\n",
    "# from imblearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import HalvingGridSearchCV\n",
    "# from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler, TomekLinks, CondensedNearestNeighbour\n",
    "# from imblearn.combine import SMOTETomek\n",
    "\n",
    "# pipeline_probability = Pipeline([\n",
    "#     ('resampler', None),  # Placeholder for resampling method\n",
    "#     ('classifier', SVC(max_iter=10000, random_state=42, probability=True))  # Model, now with probabilities\n",
    "# ])\n",
    "\n",
    "# param_grid_probability = {\n",
    "#     'classifier__C': [0.01], \n",
    "#     'classifier__coef0': [-1.0], \n",
    "#     'classifier__degree': [5], \n",
    "#     'classifier__gamma': [0.0001], \n",
    "#     'classifier__kernel': ['poly'], \n",
    "#     'classifier__tol': [0.1], \n",
    "#     'resampler': [None] \n",
    "#     }\n",
    "\n",
    "# # Set up HalvingGridSearchCV\n",
    "# halving_grid_search_probability = HalvingGridSearchCV(\n",
    "#     estimator=pipeline_probability,\n",
    "#     param_grid=param_grid_probability,  # Parameter grid remains the same  \n",
    "#     cv=5,  # 5-fold cross-validation\n",
    "#     scoring='accuracy',  # we want to optimize recall  # TODO decide with team\n",
    "#     n_jobs=-1,  # Use all processors\n",
    "#     verbose=1  # To track progress\n",
    "# )\n",
    "\n",
    "# # Fit the random search on training data\n",
    "# halving_grid_search_probability.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best parameters and score\n",
    "# print(\"Best Parameters:\", halving_grid_search_probability.best_params_)\n",
    "# print(\"Best Cross-Validation Accuracy:\", halving_grid_search_probability.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report_halving_GS_probability = classification_report(y_val, halving_grid_search_probability.predict(X_val))\n",
    "# print(\"Classification Report:\\n\", report_halving_GS_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_with_probability = halving_grid_search_probability.best_estimator_\n",
    "# save_model(model_with_probability, \"_with-probability_full-data_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the PCA datasets\n",
    "# train_pca = pd.read_csv(\"../data/pca/dataset_train_pca.csv\")\n",
    "# val_pca = pd.read_csv(\"../data/pca/dataset_val_pca.csv\")\n",
    "# test_pca = pd.read_csv(\"../data/pca/dataset_test_pca.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the PCA datasets into features and target\n",
    "# X_train_pca = train_pca.drop(columns=[\"Diabetes\"])\n",
    "# y_train_pca = train_pca[\"Diabetes\"]\n",
    "\n",
    "# X_val_pca = val_pca.drop(columns=[\"Diabetes\"])\n",
    "# y_val_pca = val_pca[\"Diabetes\"]\n",
    "\n",
    "# X_test_pca = test_pca.drop(columns=[\"Diabetes\"])\n",
    "# y_test_pca = test_pca[\"Diabetes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the classifier\n",
    "# model_pca = SVC(C=100, kernel='linear')  # result of grid search TODO\n",
    "\n",
    "# # Train the model on the preprocessed training data\n",
    "# model_pca.fit(X_train_pca, y_train_pca)\n",
    "\n",
    "# # Make predictions on the validation set\n",
    "# y_val_pred = model_pca.predict(X_val_pca)\n",
    "\n",
    "# # Calculate the accuracy\n",
    "# accuracy = accuracy_score(y_val_pca, y_val_pred)\n",
    "# print(f\"Validation Accuracy: {accuracy}\")\n",
    "# print(classification_report(y_val_pca, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA on all components has an infeasible time complexity for Support Vector Machines (at least for Salome's computer without a GPU). It needs more than 12 hours and was automatically cut off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only the best n components for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_components = 2  # TODO 5\n",
    "# X_train_best_components = X_train_pca.iloc[:, :num_components]\n",
    "# X_val_best_components = X_val_pca.iloc[:, :num_components]\n",
    "\n",
    "# # Initialize the classifier\n",
    "# model_pca_n = SVC(C=100, kernel='linear', verbose=True)  # result of grid search TODO\n",
    "\n",
    "# # Train the classifier\n",
    "# model_pca_n.fit(X_train_best_components, y_train_pca)\n",
    "\n",
    "# # Make predictions on the validation set\n",
    "# y_val_pred = model_pca_n.predict(X_val_best_components)\n",
    "\n",
    "# # Calculate the accuracy\n",
    "# accuracy = accuracy_score(y_val_pca, y_val_pred)\n",
    "# print(f\"Validation Accuracy: {accuracy}\")\n",
    "# print(classification_report(y_val_pca, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO also here: undersampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All values are computed on our validation set that was not part of the input to the cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | No Tuning (Default Hyperparameters) | Tuned on Recall | Tuned on F1-Score | Tuned on Accuracy |\n",
    "|----------|----------|----------|----------|----------|\n",
    "| Accuracy       | 0.84     | 0.16     | 0.49     | 0.25     |\n",
    "| Precision 0    | 0.84     | 0     | 0.88     | 0.96     |\n",
    "| Precision 1    | 0     | 0.16     | 0.19     | 0.17     |\n",
    "| Recall 0       | 1     | 0     | 0.45     | 0.12     | \n",
    "| Recall 1       | 0     | 1     | 0.68     | 0.97     |\n",
    "| F1 Score 0     | 0.91     | 0     | 0.60     | 0.21     |\n",
    "| F1 Score 1     | 0     | 0.27     | 0.29     | 0.29     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No Tuning**: exact same result as for Majority Class baseline. Terrible! So much training for nothing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning on Recall**: Very good for recall on positive class, but terrible for negative class -> no option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning on F1-Score**: gives best accuracy of all, recall slightly better on positive class than on negative class (what we want) although both recalls not very high (around 0.5), precision is much better for negative class than the positive one (according to precision-recall tradeoff), consequently f1 score is better for negative class than for positive class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning on Accuracy**: Interestingly, worse accuracy than when we tuned for high f1-score (due to some randomness in halving grid search?), tendency the same as for tuning on f1-score, but more extreme differences between precision and recall in both classes, thus f1-scores are worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we pick: model tuned on f1-score, since it has the best accuracies and f1-scores. Sadly, not the best recall on the positive class, but the others have their high recall on the positive class only to huge drawbacks in the positive class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
